<link rel=stylesheet href=style.css>

<h1 id="delivering-ideal-site-performance">Delivering Ideal Site Performance</h1>
<hr>
<p>What does it take to deliver web sites in the highest performing way
possible? What would it take for us to say that given certain bandwidth,
latency and CPU constraints, a web site was delivered to the user in the
best possible speed?</p>
<p>This is a question that I’ve spent a lot of time on in the last few
years, thinking about what ideal performance would take both on the
browser side as well as on the server/content side.</p>
<p>Ideal performance would require many different conditions in order to
happen:</p>
<ul>
<li>Early delivery - Content delivery needs to start as soon as possible.</li>
<li>Priorities - Critical content is delivered before less-critical one.</li>
<li>Full bandwidth pipe - The network bandwidth must be 100% used at all
times until no content delivery is required.</li>
<li>Full CPU utilization - The CPU and the browser’s main thread should be
100% used, as long as processing is required to transform the delivered
content into pixels on the screen, application behavior, and application
logic.</li>
<li>No CPU Blocking - At the same time, the browser’s main thread should not be blocked by
long execution tasks at any point in the loading process, in order to
remain responsive to user input.</li>
<li>Minimal Content - No unneeded content should be sent to the user and required content should be highly compressed.</li>
<li>Contention avoidance - Bandwidth and CPU contention should be avoided
between low priority content and high priority one, as well as between
same priority resources which need to be processed in their entirety.</li>
<li>Minimize latency impact - Use a CDN and improve content caching at the
client.</li>
<li>Control - Make sure that content and in particular third party content
does not get in the way of your user&#39;s experience.</li>
<li>Measurement - Making sure that all the above conditions are in fact true
and perform as expected.</li>
</ul>
<p>This chapter will explore strategies for satisfying these conditions
in order to avoid unnecessary delays when delivering assets on
the web. These
conditions may require changes in the way browsers load web
content. However, they cannot rely solely on that. In order for some of
these conditions to be met, web content must also be adjusted.</p>
<p>At the same time, due to limitations in size, we won’t be able to cover all the above conditions. Specifically, the chapter
will focus on the ins and outs of resource loading and will leave CPU utilization, main-thread blocking
avoidance and performance measurement aside.</p>
<p>But before we dive into the details of the difference resource loading
phases and how they can be improved, let’s take a short detour to
examine the browser’s resource loading process.</p>
<h2 id="how-browsers-load-resources">How browsers load resources</h2>
<p>A typical web page is built from multiple resource types and often many resources
that make sure the user&#39;s visual experience is a pleasant one.</p>
<p>The HTML resource is there to give the browser the structure of the
document, CSS is there to style the document and JavaScript gives it
functionality and interactivity. Beyond those, fonts make sure the
reading experience is optimal, and image, video and audio resources
provide the user with visuals as well an audio often required to get the
full context of the page.</p>
<p>But all these resources are not declared in a single place when the
browser initially fetches the HTML, when the user clicks on a link or
types something into their URL bar. All the browser has to go on at that
point in the HTML itself, and only once it fetched it does it know of
some of those other resources required for the full site&#39;s experience.</p>
<p>So what does happen when the browser navigates to a new page?</p>
<p>First, the browser starts the connection establishment process we
discussed earlier, with DNS, TCP and TLS (which often costs us around 4
RTTs). Then, it sends out a GET request for the HTML itself, and
receives an initial buffer of the response from the server.
The size of that response depends both on the HTML size and the initial
congestion window of the server. A typical value for that is window is
10 packets of 1460 bytes, or around 14KB of data.</p>
<h3 id="html-processing">HTML processing</h3>
<p>One of the great things about HTML as a format is that it&#39;s a
progressive one. It can be processed as it comes off the network, so
even if the HTML the browser has is incomplete (as is often the case),
the parts that already arrived can be processed and acted upon.</p>
<p>Now, the browser needs to process that HTML in order to start building the DOM tree from it, and most importantly for our resource-loading interests, to figure out which
resources are needed for it to fully construct and render the page. The
HTML processing phase starts by the tokenization phase - 
The browser breaks apart the HTML string into tokens, where each token
represents a tag start, tag end, or the text between the tags.</p>
<p>The tokens are just data
structure representation of the HTML&#39;s text, bringing it one step closer
to something that can be properly handled by the HTML parser.
They may represent invalid HTML, as the example below. That is expected, as
it&#39;s not the tokenizer&#39;s role to enforce HTML parsing rules.</p>
<figure>
![](media/tokenization.svg)
<figcaption>Invalid HTML gets tokenized - the `</img>` tag creates an
"EndTag" token, despite it being invalid.</figcaption>
</figure>

<p>After the tokenization phase, the browser can start using those tokens
in order to kick off resource loads. Since blocking scripts can cause
the DOM creation, it&#39;s better not to wait to load resources until the DOM is
ready.
Instead, the browser is using its <a href="https://calendar.perfplanet.com/2013/big-bad-preloader/">preloader</a> in order to
scan through the tokens and figure out resources that are highly likely to be requested later on, and kick off those requests ahead of time.</p>
<p>After the preloader ran its course, the browser uses those same tokens and parses
them according to HTML&#39;s parsing rules.  The result is
DOM nodes, interconnected into a DOM tree.
Any tokens which represent
invalid markup will be processed and used to create perfectly valid DOM.</p>
<h3 id="the-critical-rendering-path">The critical rendering path</h3>
<p>We&#39;ve seen above how the browser discovers resources, but not all
resources are created equal. In particular there&#39;s a set of resources
that the browser requires in order to initially render the web page&#39;s
content.</p>
<p>We talked about the creation of the DOM tree, and it is in fact
required in order for the browser to be able to render the page to
screen, but unfortunately it is not sufficient. In order to be able to
initially render the page with its appropriate styling to the
user (and avoiding a &quot;Flash of unstyled content&quot;), the browser also
needs to create the Render Tree and in most cases the CSSOM.</p>
<aside>
The CSS Object Model (CSSOM) is a set of APIs that
enable scripts to examine the styles of various DOM nodes, as well as
setting them to change their appearance.
</aside>

<p>In order to create both the CSSOM and the render tree, the browser needs
to download all the external CSS resources and evaluate them. Then it
processes those downloaded rules, along with
any inline style tags and style attributes in order to calculate which
styles (if any) are applied to each node in the DOM tree.</p>
<p>This is also where blocking scripts come in. Blocking scripts (as their
can suggest) block the HTML parsing as soon as they are discovered and
HTML parsing does not continue until they finish being downloaded,
parsed and executed. One more detail about them is that, at least in
many cases, they cannot
start executing until all CSS that preceded them finished being
downloaded and processed.</p>
<p>The reason for that is that those scripts can access the bits of the DOM
that are already generated and query their CSSOM. If they do that, the
CSSOM state must be stable and complete. Hence browsers must make sure
that this is the case, and they do that by downloading and processing
all CSS before any CSSOM reads. The result of that can be a lot of delay
to the JS execution, which in turn delays the DOM creation. Sadness.</p>
<p>To top all that, the above is applicable to both external blocking
scripts as well as inline ones. So inline scripts can and will block
your HTML processing if they are preceded by external stylesheets.</p>
<p>Now, once the browser have built a sufficiently large DOM tree and calculated the styles for each DOM node, it can
walk over the DOM and create the render tree, by creating an equivalent
tree for each node that actually partake in the page&#39;s rendering and is
displayed on screen.</p>
<p>After that, the browser can proceed to laying out the different elements
on the screen and eventually paint them.</p>
<p><img src="media/critical_path.jpg" alt=""></p>
<h3 id="request-destination-and-credentials-mode">Request destination and credentials mode</h3>
<p>One important thing to understand about loaded resources in the browser
is that each request has its own destination as well as credentials
mode.</p>
<h4 id="request-destination">Request destination</h4>
<p>The request&#39;s <a href="https://fetch.spec.whatwg.org/#concept-request-destination">destination</a> indicates what &quot;type&quot; of resource we&#39;re
expecting to receive from the server and how that resource will be used.
Different destinations include &quot;stylesheet&quot;, &quot;script&quot;, &quot;image&quot;, &quot;font&quot;
and more. A requests&#39; destination has several implications on resource
loading. As we&#39;ll discuss later, it is used in determining the priority
of the request. It is also used in various internal caches in the
browser to make sure a previous response can be served to a current
request only when their destinations match.</p>
<h4 id="credentials-mode">Credentials mode</h4>
<p>A request&#39;s <a href="https://fetch.spec.whatwg.org/#concept-request-credentials-mode">credentials mode</a> determines whether the request gets
credentials information (cookies in the typical case) sent along with it
to the server. Possible values can be:</p>
<ul>
<li>&quot;include&quot; - indicating that a request will have credentials</li>
<li>&quot;omit&quot; - indicating that it will not have credentials</li>
<li>&quot;same origin&quot; - indicating that a request will have credentials only
when sent to the same origin.</li>
</ul>
<p>Each resource type may have different credentials mode by default.
Resource fetches triggered by <code>&lt;img&gt;</code> and <code>&lt;script&gt;</code> will have a default
credentials mode of &quot;include&quot; by default, while font fetches will have a
&quot;same origin&quot; credentials mode. <code>fetch()</code> calls used to have a default
&quot;omit&quot; mode, but recently changed to &quot;same origin&quot;.</p>
<p>All that to say that as a performance-aware web developer, you need to
pay close attention to the credentials mode of the resources you&#39;re loading.</p>
<p>In markup, developers can also change the credentials mode of the
resources they load by including the <a href="https://html.spec.whatwg.org/multipage/urls-and-fetching.html#cors-settings-attributes"><code>crossorigin</code></a> attribute, which
modifies the default for markup-based resource fetches:</p>
<ul>
<li>&quot;anonymous&quot; or an empty value indicates a &quot;same origin&quot; credentials
mode.</li>
<li>&quot;use-credentials&quot; indicates an &quot;include&quot; credentials mode.</li>
</ul>
<p>Note that there&#39;s no current value which results in an &quot;omit&quot;
credentials mode.</p>
<p>Why does the credentials mode matter?</p>
<p>First of all, you may expect cookies on the server for a certain
request and understanding credentials mode will help you understand when
they&#39;d be there and when they will not be.</p>
<p>Otherwise, similarly to the request
destination, the credentials mode determines if a request and a response
can get matched in the browser&#39;s internal caches. On top of that, some
browsers (most notably Chrome) will use separate connections for
requests of different credential modes. We&#39;ll expand later the impact of
that.</p>
<h3 id="all-set-">All set?</h3>
<p>Now with a better understanding of the way browsers load resources,
let’s take a look at the different conditions required to make that
loading as fast as possible.</p>
<h1 id="early-delivery">Early Delivery</h1>
<p>When it comes to web performance, our goal is usually seemingly obvious:
we want <em>meaningful</em> content to be accessible as fast as possible. As a
part of it, we need resources that are part of the critical path to be delivered early.</p>
<p> How early is early enough? Probably shortly after the browser sent out a request for
it, typically after the user clicked on a link or typed something in
their URL bar. (or even before that, if the application can have high
enough confidence that they would — e.g. when the user moves their mouse pointer within
a certain proximity of a button)</p>
<h2 id="protocol-overhead">Protocol overhead</h2>
<p>Network protocols introduce overhead to network communication. That
overhead is manifested in extra bytes - while most of our network
traffic is content, some part of it is just protocol information
enabling the delivery of that content.
But the overhead is also manifested in time - establishing a connection
or passing along critical information from client to server or vice
versa can take multiple Round-Trip-Times (or RTTs), resulting in those
parts of the delivery being highly influenced by the network&#39;s latency.</p>
<p>The result of that is a delay until the point where the browser
starts receiving the response from the server (also known as
Time-To-First-Byte or TTFB). Recent
advances in the underlying protocols make that less of an issue, and
can reduce TTFB significantly.</p>
<h3 id="0-rtt">0-RTT</h3>
<p>In their non-cutting-edge version, the DNS, TCP and TLS protocols require a large number of
RTTs before the server is able to start delivering useful data to the user.</p>
<p><img src="media/dnstcptls.svg" alt=""></p>
<p>Protocol advancements such as QUIC on the one hand and TCP-Fast-Open
(TFO) and TLS1.3 on the other hand, made most of that obsolete. These protocols
rely on previous established connections in order to keep cryptographic
&quot;cookies&quot; that remember past sessions, and allow previous sessions to be
recreated instantaneously.
There are still many caveats and many scenarios where the connection
will take more than a single RTT to be established, but generally, using those cutting edge protocols
will mean that there are very few RTTs which get in the way of content delivered to our users.</p>
<aside>

<h4 id="quic-quick-udp-internet-connection">QUIC - Quick UDP Internet Connection</h4>
<p>HTTP/2 as a protocol brought many necessary improvements to resource
delivery over HTTP. HTTP/1 before it had a severe head-of-line blocking
issue, where the browser could have sent in practice only one request at
a time on a connection. That created a linear connection between the
number of requests a page had and the number of RTTs it took for the
page&#39;s resources to be downloaded. As a result, browsers settled on
opening multiple connections per host (typically 6 connections for
modern browsers), to reduce the latency by a fixed factor.</p>
<p>HTTP/2 fixed that by enabling multiplexing of multiple request and
responses over a single connection, without incurring extra latency
costs per request. As a result, for HTTP/2 browsers only use a single
connection per host. But a side effect of that is that HTTP/2 delivery
is more sensitive to packet losses than HTTP/1, as each packet loss
impacts <em>all</em> the resources, rather than just one of them.</p>
<p>As a result, in highly-lossy environments, there are scenarios where
HTTP/2 ends up being slower than HTTP/1, making the switch to HTTP/2
tricky for sites that have large chunks of their audience coming in from
bad networking environments.</p>
<p>In order to resolve that, Google Chrome folks started exploring ways to
make the transport layer more aware of HTTP/2&#39;s request-response streams.
Since TCP is widely deployed and inspected by intermediary network
components, it&#39;s practically impossible to significantly change the
protocol without some of those network components breaking that traffic
in the process. Re-implementing a reliable transport layer over a
different transport protocol, UDP, was a simpler option.</p>
<p>The result is QUIC (standing for Quick USP Internet Connections) - a new
protocol combining the transport, encryption and application layers all
into a single, coordinated processing model.</p>
<p>This new protocol have several shiny new improvements over the protocols
of yore:</p>
<ul>
<li>0-RTT - combining layers means that the protocol needs to
establish a connection once (for both transport and encryption
layers). On top of that, once a user has established a connection to a
server once, the client can remember those encryption credentials and
resume that same connection in the future with 0-RTTs (sending those
past-credentials along with the request).</li>
<li>Stream-aware loss handling - Another major advantage of merging the
layers is in handling of packet losses. Both TCP and QUIC offer
reliable transport, which means that when a packet is lost, it has to be
successfully retransmitted before the following packets can be delivered
to higher layers. However, TCP guarantees reliable delivery for the
entire connection, while QUIC guarantees reliable delivery <em>per
request-response stream</em>. That means that if a packet is lost in QUIC,
it will only delay the impacted resources, while HTTP/2 over TCP will
delay <em>all resources</em>.</li>
<li>Tighter prioritization - Because QUIC is implemented over UDP, the
transport layer logic and queues are all implemented in the server in userland code, rather than in the kernel, where TCP&#39;s logic is implemented.
As a result, it&#39;s easier for the server to have tighter controls over
the priorities of the resources that are currently in the sending
queues. One of the problems with the HTTP/2 over TCP model is that the
TCP queues are outside of the server&#39;s control. When the server is
sending down a mix of resources in different priorities, there can be
situations where the TCP queues already have not-yet-sent lower priority
resources, which will then delay the high-priority resource from being
sent. With QUIC, a server implementation can have tighter control over
the queues and make sure that lower priority resources don&#39;t get in the
way.</li>
</ul>
<p>QUIC is not yet a standard protocol. It is only implemented as part of
the Chromium project&#39;s network stack, and used as a library by Chromium, Android apps as well as servers.
At the same time, there are significant efforts at the IETF - the Internet Engineering Task
Force - to create a standard version of the protocol, so that it can be implemented independently in an interoperable way across multiple browsers and servers.</p>
</aside>

<h3 id="preconnect">Preconnect</h3>
<p>Preconnect is browser hint which tells it the page is about to download
a resource from a certain host, enabling it to connect to it ahead of time.
It is expressed as a <code>rel</code> attribute on a <code>&lt;link&gt;</code> element. For example,
<code>&lt;link rel=preconnect href=&quot;https://www.example.com&quot;&gt;</code> tells the browser
to create a connection to the host <code>www.example.com</code> even if it hasn’t
encountered any resources on that host just yet.</p>
<p>That is another way to get rid of these pesky RTT - just get them
out of the way sooner, so that they don&#39;t get in the way of your site&#39;s
critical rendering path.</p>
<h3 id="adaptive-congestion-window">Adaptive congestion window</h3>
<p>When the server starts sending data to the user, it doesn&#39;t know how much bandwidth it will have for that purpose. It can&#39;t be sure what
amount of data will overwhelm the network and cause congestion. Since
the implications of congestion can be severe, a mechanism called <em>Slow Start</em> was developed in order to make sure it does not happen.
It enables the server to gradually discover the connection&#39;s limits, by starting to send a small amount of packets, and increasing that exponentially.
But due to its nature, slow start also limits us in the amount of data that we can initially send on a new connection.</p>
<aside>
#### Congestion collapse
The reason slow start is needed is to avoid congestion collapse.

<p>Let&#39;s imagine a situation where the server is sending down significantly more packets
than the network can effectively deliver or buffer. That means that many
or even most of these packets will get dropped. And what do reliable
protocols do when packets are dropped? They retransmit them!!</p>
<p>So now the server is trying to send all the retransmits for the dropped
packets, but they are <em>still</em> more than the network can handle, so they
get dropped.</p>
<p>What does the helpful transport layer do? You guessed it, retransmit
again.</p>
<p>That process happens over and over, and ends up with a network which
passes along mostly retransmits and very little useful information.</p>
<p>This is the reason it pays off to be conservative, start with a small
number of packets and go up from there.</p>
</aside>

<p>In the past the recommended amount of packets for the initial congestion
window - the initial amount of packets the server can send on a fresh
connection - was <a href="https://tools.ietf.org/html/rfc3390">somewhere between 2 and 4, depending on their
size</a>.</p>
<p>In 2013 a group of researchers at Google ran experiments all over
the world with different congestion windows and <a href="https://tools.ietf.org/html/rfc6928">reached the
conclusion</a> that this value can be lifted to 10.</p>
<p>If you&#39;re familiar with the &quot;send all your critical content in the first
14KB of your HTML&quot; rule, that&#39;s where this rule is coming from. Since
the maximum Ethernet packet size is 1460 bytes of payload, 10 packets
(or ~14KB) can be delivered during that first initial congestion window.
Those packets need to include both the headers and the critical content
payload if you want your site to reach first paint without requiring
extra round-trips.</p>
<p>However, in many cases the network over which we&#39;re connecting can
handle significantly more than that, and theoretically the server could
have known that in some cases (since they&#39;ve seen this browser over that
network before, etc).</p>
<p>The browser itself can also, in many cases, take an educated guess
regarding the quality of the network it&#39;s on. It can do that based on
the network radio type, signal strength, as well as past browsing
sessions on that network.</p>
<p>Browsers already expose network information to JavaScript through the <a href="https://wicg.github.io/netinfo/">Network Information API</a>, and there&#39;s work underway
to expose the same information using Client-Hints. That will enable
servers to modify their initial congestion window based on the <a href="https://wicg.github.io/netinfo/#-dfn-effectiveconnectiontype-dfn-enum">Effective
Connection Type</a> and adapt it according to the browser&#39;s estimate.
That would help to significantly minimize the time it takes the
connection to converge onto its final congestion window value.</p>
<p>The advent of QUIC as the transport protocol will also make this easier
to implement on the server, as there&#39;s no easy way to increase the
congestion window for an in-flight connection in TCP implementations today.</p>
<h3 id="what-to-do-">What to do?</h3>
<p>How can you make sure that you&#39;re minimizing your protocol overhead?
That depends on your server&#39;s network stack.
Moving to QUIC, TLS/1.3 or TFO may require upgrading your server&#39;s network stack, or
turning on those capabilities in its configuration. Alternatively, you can use a Content-Delivery Network (or CDN) which supports those protocols, and have servers which sit between your users and your servers.</p>
<p>These protocol enhancements are all relatively new, so support may not be widespread at the moment.
QUIC specifically is still not standardized, so it can be hard to self
deploy it, as that would require frequent updates as the Chrome
implementation evolves.
At the same time, keeping an eye for these protocols and taking
advantage of them once they become available can be beneficial.</p>
<p>Alternatively, it is easier to turn on preconnect for your critical third party hosts,
by adding <code>&lt;link rel=preconnect href=example.com&gt;</code> to your markup. One
caveat here is that preconnects are cheap, but not free. Some browsers
have a limited number of DNS requests that can be up in the air (e.g.
Chrome limits pending DNS requests to 6).</p>
<p>Preconnecting to non-critical hosts can use up your quota, and prevent
connections to other hosts from taking place. So only preconnect to
hosts your browser would need to preconnect to, and prefer to do that
roughly in the same order as the browser would use those connections.</p>
<p>When using preconnect, you also need to pay close attention to the
credentials mode of the resources will use that connection. Since Chrome
uses different connections for different credential mode requests, you
need to let the browser know which connection “type” it needs to
preconnect to. You can do that with the <code>crossorigin</code> attribute on
preconnect <code>&lt;link&gt;</code> element.</p>
<p>And as far as adaptive congestion window goes, that requires some more
server-side smarts, but hopefully with the advent of network info in
Client-Hints and QUIC, one can imagine servers implementing that scheme.</p>
<h2 id="server-side-processing">Server-side processing</h2>
<p>The other hurdle to overcome when it comes to early delivery is server
side processing. HTML content is often dynamic and as such, generated on
the server rather than simply delivered by it. The logic for its generation
can be complex, may involve database access and can also include
different APIs from different services. That processing is also often
done using interpreted languages such as Python and Ruby, which are not
always the fastest choice, at least not by default.</p>
<p>As a result, generating the HTML once a request has hit our servers is a
time consuming activity, and one which wastes our users&#39; time. Worse
than that, it is time spent before the browser can do anything to
advance the loading of the page.</p>
<h3 id="early-flush">Early Flush</h3>
<p>One way to avoid forcing our users to wait on our potentially slow
database responses is to flush the HTML&#39;s <code>&lt;head&gt;</code> early on. As part of the HTML
generation process we can make sure that once the <code>&lt;head&gt;</code> is ready, it will be sent down to the browser, enabling it
to start processing it, issue the requests defined in it and
create the relevant DOM tree.</p>
<p>However that is not always feasible in practice. Depending on your
application logic, there are a few problematic aspects about early
flushing your HTML content:</p>
<h4 id="committing-an-http-response-code">Committing an HTTP response code</h4>
<p>If the eventual response will not be a <code>200 OK</code> one (e.g. a 404 or a 500 error if the
server failed to find the required info in the database), you would have
to redirect the content using JavaScript. Since not all search crawlers
support JavaScript, it can mean that such &quot;eventual error&quot; pages can now
find themselves in search results. It also means we need to bake that &quot;eventual error&quot; logic into our
client side application logic, in order to make sure the negative user impact of
that redirection is as unnoticeable as it can be.</p>
<h4 id="dynamic-response-header-logic">Dynamic response header logic</h4>
<p>Any dynamic logic applied to the response headers must be applied
  ahead of time. Header based cookies, Content-Security-Policy directives and other header-based instructions  have to be
determined before that first content flush happens. In some cases that
can delay early flushing of content, in others it can prevent it
entirely. (e.g. if the cookie that is supposed to be set is itself
coming from a database)</p>
<p>Alternatively, you can try to build in application logic that converts
these header based browser instructions into content-based ones (e.g.
set the cookies using JS, set some headers using meta tags as part of
the content itself, etc.).</p>
<h4 id="compression-buffering">Compression buffering</h4>
<p>Even if your application logic enables you early flushing of your HTML&#39;s head,
misguided gzip or brotli settings can result in undesired buffering. When done wrong, they can
nullify your attempts to send content early to the browser. Gzip and
brotli have various settings which control the trade-off between
buffering content and compression ratio - the more buffering, the better
compression ratio these algorithms can achieve, but at the cost of extra
delays. When flushing early, you should make sure that your server (and
whatever other component in your infrastructure which is applying
compression) is set accordingly.</p>
<p>For <a href="https://www.zlib.net/manual.html">gzip</a>, its default buffering mode is <code>Z_NO_FLUSH</code>, which
means it is buffering data until it considers it has enough to create
ideal compression output. Setting it to <code>Z_SYNC_FLUSH</code> can ensure that
every input buffer creates an output buffer, trading off some
compression ratio for speed.</p>
<p>If you&#39;re using on-the-fly brotli to compress your content, there&#39;s a
similar flag in the streaming brotli encoder, called
<a href="https://github.com/google/brotli/blob/1e7ea1d8e61b7cd51149a2dd491bc86ff8ef460c/c/include/brotli/encode.h#L90"><code>BROTLI_OPERATION_FLUSH</code></a>. You should use it instead of
the default.</p>
<h3 id="server-push">Server Push</h3>
<p>Another way to work around slow server-side processing is the use an
HTTP/2 (or H2, for short) mechanism called <em>Server Push</em> in order to load critical resources
while the HTML is being generated.</p>
<p>That often means that by the time the HTML was generated and started to
be sent to the browser, the browser already has all the critical
resources in its cache.
Furthermore, sending of those critical resources start the TCP
slow-start process earlier and ramps up TCP&#39;s congestion window. When
the HTML is finally ready, a significantly larger part of it can be sent down.</p>
<figure>
![](media/page_loading_nopush.svg)
<figcaption>Loading process without push - HTML download and slow start
kick off only after HTML is generated.</figcaption>
</figure>
<figure>
![](media/page_loading_push.svg)
<figcaption>Loading process with push - critical resources and slow start
kick off before HTML is generated, so once it is ready, it can be sent
down in a single RTT.</figcaption>
</figure>

<p>One of the biggest problems with Server Push today is that the server
has no visibility into the browser&#39;s cache state, so when done naively,
the server is likely to send down resources that the browser already has
in its cache.
A recent proposal called <a href="https://tools.ietf.org/html/draft-ietf-httpbis-cache-digest-04">Cache Digests</a> is aiming to solve that, by
sending the server a condensed list of all the resources the browser has
in its cache for that particular host, enabling the server to take that
into consideration before pushing down resources.</p>
<p>In practice, because of the caching issues, you don&#39;t want to use server
push naively.
One easy way to implement server push is to make sure it&#39;s only in
effect on your landing pages, and only working for first views.
You can use a cookie to distinguish first views from repeat visits, and
use the request&#39;s <code>Accept</code> headers to distinguish navigation requests
from subresource ones.</p>
<p>Actually triggering server push varies per server: some servers use Link
preload headers as a signal to push resources. The problem with that
approach is that it doesn&#39;t take advantage of the server&#39;s &quot;think time&quot;.
An alternative approach of configuring the server to issue pushes based
on the arriving requests can prove more beneficial, as it triggers the
push earlier.</p>
<p>If you want to go all in, there&#39;s also a <a href="https://github.com/h2o/cache-digest.js/blob/master/README.md">Service Worker based
implementation of Cache Digests</a>, which can help you
deliver push on all your pages today, without waiting for Cache Digests
to be standardized and implemented.</p>
<h3 id="early-hints">Early Hints</h3>
<p>Yet another alternative is a newly adopted HTTP standard header called
<a href="https://tools.ietf.org/html/rfc8297">Early Hints</a>. (assigned with the response number 103)
That header enables the server to send an initial set of headers that
will be indications regarding the resources that a browser should load,
while still not &quot;locking down&quot; the final response code.</p>
<p>That would enable servers to send down Link preconnect and preload
headers in order to tell the browser as early as possible which
hosts to connect to and which resources to start downloading.</p>
<p>The main downside here is that Early Hints is not yet implemented by any
browser, and their implementation may not be trivial. Because the hints
are received before the final HTTP response, the browser doesn&#39;t
necessarily have in place the rendering engine process that will end up
processing that document. So supporting Early Hints would require
supporting a new kind of requests that are not necessarily triggered by
the rendering engine, and potentially a new kind of cache that will keep
these resources around until the rendering engine is set up and received
the final headers and document.</p>
<h3 id="prefetch">Prefetch</h3>
<p>Finally, there&#39;s also the option of working around the server&#39;s &quot;think
time&quot; by kicking it off significantly earlier, before the user even expressed
explicit interest in the page.
Browser use past user activity in order to predict where the use is
likely to go next and prefetch those pages while they are typing in the
address, or even before that.</p>
<p><code>&lt;link rel=prefetch&gt;</code> is an explicit way in which developers can do the
same and tell the browser to start fetching an HTML page or critical
resources ahead of time, based on application knowledge.</p>
<p>Those fetches remain alive when the user actually navigates to the next
page, and remain in the cache for a limited amount of time even if the
resource is not cacheable.</p>
<p>If you have a good-enough guess of where your users will be heading
next, prefetch can be a good way to make sure that when they do, the
browser will be one step ahead of them.</p>
<h1 id="priorities">Priorities</h1>
<p>Since the download process of a web page is comprised of downloading
dozens (and sometimes more) resources, properly managing the download priority of these
resources is extremely important. We discussed the resource loading
process earlier. A part of that process is determining each resource’s
priority.</p>
<h2 id="request-priorities">Request priorities</h2>
<p>Once the browser detected a resource that it needs to load, it kicks off
a request that will fetch that resource.
We&#39;ve seen earlier that some requests may be more important than
others. Resources that are in the critical path should ideally reach
the browser before ones that are not in the critical path and that are
required at a later phase of the rendering process.</p>
<p>How do browsers make sure that these critical requests get higher
priority over less-critical ones?</p>
<p>We&#39;ll talk about more sophisticated priority schemes later, but if we
look at HTTP/1.X, the only method for browsers to control the priority
of a specific request is by... not sending it up to the server in the
first place. Browsers have a request queue of requests that were discovered
but should not yet be sent up to the server because there are more
critical resources in flight, and we don&#39;t want the less-critical ones
to contend on bandwidth with them.</p>
<p>Different browsers use different schemes there: some hold off all
non-critical requests while critical ones are in flights, while other
let through some non-critical requests to go out in parallel to critical
ones, to improve image performance.</p>
<p>Browsers assign priorities to resources according to their &quot;type&quot; (more strictly speaking, according to their <code>Request.destination</code>).
In the priority queue model, they make sure that a request doesn&#39;t go
out to the network before all requests from all priorities above it were
not sent out to the network.</p>
<p>Typically, browsers will assign higher priorities to rendering-critical resources. In Chrome that translates into the following order:
HTML, CSS, fonts, JS, in-viewport images, out-of-viewport images.
Chrome also follows some further heuristics which go beyond what other
browsers typically do:</p>
<ul>
<li>Blocking scripts at the bottom of the page
are of lower priority than blocking scripts at the top of the
page.</li>
<li>Async and defer scripts are of even lower priority</li>
<li>Preloaded fonts get slightly lower priority than late-discovered
fonts, to prevent them contending with render blocking resources.</li>
</ul>
<p>The purpose of these heuristics is to try to infer the developer&#39;s
intent from various existing signals: the location of the resource on
the page, whether it is a blocking resource, etc.
However, that approach has its limits, and it&#39;s hard for the developer
to convey their real intent to the browser.</p>
<p>Which is why we need...</p>
<h2 id="priority-hints">Priority Hints</h2>
<p>We&#39;ve seen before that browsers assign different priorities to different
resource types based on various heuristics. But as much as these
heuristics have done well over the years to optimize content download,
they are not accurate in all cases, and are based on indirect
indications from the developer regarding which resources are most
important for their site. The <a href="https://wicg.github.io/priority-hints/">Priority Hints</a> proposal
is an attempt to make those developer indications explicit, and let the
browser take action based on them.</p>
<p>The proposal aims for developers to include new &quot;importance&quot; attributes
to various resource downloading HTML elements, which will indicate
whether the resource&#39;s priority should be upgraded or downgraded.</p>
<p>That would give browsers a clear signal on how to prioritize a resource
vs. its counterparts, without binding them to direct developer
instructions, which may not be accurate (e.g. a blocking script with
&quot;low&quot; importance is still blocking layout, and therefore should not have
its priority downgraded).</p>
<p>At the time of this writing, the proposal is to add an <code>importance</code>
attribute on <code>&lt;link&gt;</code>, <code>&lt;img&gt;</code>, <code>&lt;script&gt;</code> and <code>&lt;iframe&gt;</code> elements, with
possible values of <code>low</code> and <code>high</code>. The hints will enable developers to
inform the browser about low priority resources (e.g. non-critical JS)
as well as high priority ones (e.g. hero images).</p>
<p>A similar parameter will also be added to the <code>fetch()</code> API options,
enabling setting the hint on dynamically generated requests.</p>
<h2 id="stream-internal-priorities">Stream internal priorities</h2>
<p>One of the assumptions behind HTTP/2&#39;s prioritization scheme is that
every resource has its priority. And that assumption works well for
resources which have to be processed as a whole such as CSS and
JavaScript. These resources are either critical or not, in their
entirety.</p>
<p>However, for streaming resource types, such as HTML and images, that
assumption doesn&#39;t necessarily hold.</p>
<p>If we consider HTML, its first few bytes which contain the <code>&lt;head&gt;</code>,
enable the document to be committed and the critical resources to be
requested are of the highest priority. At the same time, especially for
long HTML files which go beyond the initial viewport, the last few bytes
which are required to finish constructing the DOM, but are not required
for the initial rendering, are of lower priority.</p>
<p>Similarly for images, the first bytes which contain the image&#39;s
dimension are extremely important as they enable the browser the reserve
space for the image and enable layout to
stabilize.
For progressive images, the first byte range which contain the
first scan are significantly more important than the last bytes of the
last scan. The former enable the browser to display the rough image (in
low quality), while the latter provides small quality improvements.</p>
<p>So download schemes which would enable us to download the first scans of
all in-viewport images provide a significantly better user experience
than ones which download images one-by-one.</p>
<p><img src="media/progressive_download.png" alt=""></p>
<!-- Credit John Mellor -->

<p>Currently HTTP/2 priorities don&#39;t enable us to include such
prioritization schemes into the protocol.
And even if it did, there&#39;s currently no way for the browser to know
the exact byte ranges of the various images&#39; before it started
downloading them.
But that&#39;s potentially something we could improve in the future.</p>
<p>In the meantime, smart servers could do that by overriding the
priorities sent by the browser.</p>
<h1 id="delivering-the-right-content-at-the-right-time">Delivering the right content at the right time</h1>
<p>Since the browser discovers the content progressively and content has
different priorities, it is important to make sure that the application
including the content does so in a way such that the right content is
loaded at the right time, and critical and non-critical content are not
mixed together.</p>
<h2 id="css-critical-and-non-critical">CSS - critical and non-critical</h2>
<p>Loading only the critical bits of your CSS upfront, while lazy loading
the later-needed bits can be one of the most impactful optimizations
your can apply to speed up your first paint and first meaningful paint
rendering metrics.</p>
<p>Your CSS can probably be divided into three parts:</p>
<ul>
<li>Critical CSS - CSS required to style elements which are present in
the initial viewport.</li>
<li>Non-critical CSS - CSS required to style elements outside of the
initial viewport or in other parts of your site.</li>
<li>Unused CSS - CSS rules which are not used anywhere on your site.</li>
</ul>
<p>With the advent of front-end CSS frameworks, such as Bootstrap, many
sites are downloading significantly more CSS than they actually use.
More often than not, they are loading it upfront, by simply add <code>&lt;link
rel=stylesheet&gt;</code> tags into their markup.</p>
<p>When the browser&#39;s preloader sees such tags, or when the equivalent
elements get added to the browser&#39;s DOM tree, this CSS is downloaded at
a high priority as it is considered critical.</p>
<p>In terms of processing, since CSS is applied in a cascaded fashion,
before it can be processed to calculate the page&#39;s styles, all the CSS
code needs to be parsed and taken into account. That includes not just
the CSS from a single file, but from all of your included CSS files - at
least the ones in your document <code>&lt;head&gt;</code>.</p>
<p>The reason for that is that any CSS rule down at the bottom of your CSS
can override any rule at the top of it, so in order to know which rules
actually apply, all of your CSS has to be processed.</p>
<p>Therefore it is highly recommended that you will only deliver upfront your
critical CSS - the CSS absolutely needed for your page&#39;s initial render.
All other CSS should be loaded asynchronously, in a non-render-blocking
fashion.</p>
<p>And obviously, it&#39;s best to avoid sending down CSS you won&#39;t need.</p>
<h3 id="how-to-deliver-critical-css">How to deliver critical CSS</h3>
<p>So how do we deliver this critical CSS? What the best way to send it
down to the browser?</p>
<p>Well, the simplest option is simply to include a <code>&lt;link rel=stylesheet&gt;</code>
tag in your markup and let the browser discover that CSS file and
download it. While that works, it&#39;s not the fastest option. The browser
has to tokenize the HTML is order to start that download, which means an
extra RTT plus some processing time before that download starts.</p>
<p>Another option is to inline that critical CSS into your HTML so that it
will get delivered as part of the same resource, saving us an RTT.
That&#39;s a better option, especially if your critical CSS is fairly small.
At the same time, your caching will suffer. Delivering the CSS inline
means that you&#39;ll be sending it down again and again for every repeat
visit on every page to your site. If the CSS is small enough it could be
worth your while, but it&#39;s a trade-off you should be aware of.</p>
<p>A third option is to use H2 push in order to deliver critical CSS before your
HTML even hits the browser.</p>
<h3 id="how-to-deliver-non-critical-css">How to deliver non-critical CSS</h3>
<p>So we&#39;ve covered how to deliver your critical CSS, but how do you
deliver your non-critical one? We&#39;ve seen earlier that adding it your
your HTML as <code>&lt;link rel=stylesheet&gt;</code> will cause the browser to think it
is blocking and hold off rendering until it&#39;s fully downloaded and
processed. How do we avoid that?</p>
<h4 id="using-requestanimationframe-as-a-first-paint-proxy">Using <code>requestAnimationFrame</code> as a first paint proxy</h4>
<p>One technique of making sure you don&#39;t load any non-critical CSS before the initial page
renders is to trigger the loading of those CSS only after that render
happens. While there&#39;s no direct event that fires when the initial
render happens, the <a href="https://developers.google.com/speed/docs/insights/OptimizeCSSDelivery">first requestAnimationFrame event</a> of the page
corresponds to that pretty well.</p>
<p>Loading of CSS at that point will not block the page&#39;s
initial rendering, so its loading will have no negative side effect.</p>
<pre><code>    var loadStyle = () =&gt; {
        var link = document.createElement(&quot;link&quot;);
        link.rel = &quot;stylesheet&quot;;
        link.href = &quot;non-critical.css&quot;;
        document.body.appendChild(link);
    };
    if (window.requestAnimationFrame) {
        window.requestAnimationFrame(() =&gt; {
            window.setTimeout(loadStyle, 0);
        }
    } else {
        window.addEventListener(&quot;DOMContentLoaded&quot;, loadStyle);
    }</code></pre><!-- Credit Google Developers documentation for the example -->

<h4 id="preload">preload</h4>
<p>An alternative technique to decouple the CSS&#39;s execution from its
loading is to use <code>&lt;link rel=preload</code> is order to trigger the CSS
loading and only process it and take its rules into account once it&#39;s
loaded. In order to do that, we can take advantage of
HTMLLinkElement&#39;s <code>load</code> element.</p>
<p><code>&lt;link rel=&quot;preload&quot; as=&quot;style&quot; href=&quot;async_style.css&quot; onload=&quot;this.rel=&#39;stylesheet&#39;&quot;&gt;</code></p>
<!-- Credit Scott Jehl for original technique -->

<p>You can also combine the two methods above in order to trigger a
download of the preload resource, only after the document was painted in
order to avoid having it contending on bandwidth with more critical
resources.</p>
<p>The downside of these two methods is that the resource will be
downloaded with high-priority, so it’s running a real risk of contending
with other resources on the page.</p>
<h4 id="inapplicable-media-attribute">Inapplicable media attribute</h4>
<p>Another way to trigger an early download of a CSS resource without
triggering its execution is to use the <code>media</code> attribute of
<code>HTMLLinkElement</code> with the <code>stylesheet</code> <code>rel</code> attribute.
Such resources are being downloaded at a lower priority by the browser,
yet not executed until the download has finished.</p>
<p>An example may look something like this:
<code>&lt;link rel=stylesheet href=&quot;async_style.css&quot; media=&quot;not all&quot; onload=&quot;this.media=&#39;all&#39;&quot;&gt;</code></p>
<h4 id="progressive-css-loading">Progressive CSS loading</h4>
<p>The above methods all work but are not necessarily easy to use, as they
require extra work in order to load your CSS in a performant way.</p>
<p>What if we could change the way CSS behaves to make it easier to
progressively load our CSS, and have it only block the rendering of the
contents below it?</p>
<p>That would enable us to <a href="https://jakearchibald.com/2016/link-in-body/">load our CSS whenever we actually need
it</a>, and the browser would just do the right thing.</p>
<p>Up until recently, that was (more or less) the way Firefox, Safari and
Edge loaded CSS, but not Chrome. Chrome and other Chromium based
browsers blocked the rendering of the entire page as soon as an external
CSS resource was discovered, even if it was in the body. That meant that
using that technique would have had a significant performance penalty in
Chrome.</p>
<p>But starting from Chrome 69, Chrome aligned its behavior. This means
that developers can now include non-critical CSS, using <link
rel=stylesheet> tags inside their content, without it having a negative
impact.</p>
<p>One caveat is that Firefox doesn&#39;t always block rendering for in-body
style sheets, which can result in Flash Of Unstyled Content (or FOUC). A
way to work around that is to include a non-empty script tag after the
<code>&lt;link&gt;</code> tag. That forces Firefox to block rendering at that point until
all styles have finished downloading.</p>
<p>An example of the above would be:</p>
<pre><code class="language-html">&lt;html&gt;
  &lt;head&gt;
  &lt;style&gt;/* Critical styles */&lt;/style&gt;
  &lt;/head&gt;
  &lt;body&gt;
  &lt;!-- Critical content --&gt;
  &lt;link rel=stylesheet href=foo.css&gt;
  &lt;script&gt; &lt;/script&gt;&lt;!-- Note the space inside the script --&gt;
  &lt;!-- Content which is styled by foo.css --&gt;
  &lt;/body&gt;
&lt;/html&gt;</code></pre>
<p>A future improvement might be for browsers to adapt the CSS priority to
its location, as it turns it from a render-blocking resource to a
partial blocker.</p>
<!-- Credit Jake Archibald and Pat Meenan for research, technique and implementation -->

<h4 id="which-one-should-i-use-">Which one should I use?</h4>
<p>We discussed 4 different techniques to load CSS, but when should you
use each one of them?</p>
<p>These techniques differ in the time in which the requests are triggered
as well as in the request’s priority. Also, some of them rely on script
execution which, some argue, goes against the principles of
progressive enhancement.</p>
<p>At the end, I believe the easiest and cleanest technique to load
non-critical styles that may be needed further down the page is to
include them as link tags in the body, and have them block only the
content that relies on them.
If you have styles that are only needed for follow-up pages, you may use
“inapplicable media” technique, but arguably, <code>&lt;link rel=prefetch&gt;</code> may
be a better tool for the job.</p>
<h2 id="js-critical-and-non-critical">JS - critical and non-critical</h2>
<p>Similarly to CSS, blocking JavaScript also holds off rendering. Unlike
CSS, JavaScript <a href="https://medium.com/dev-channel/the-cost-of-javascript-84009f51e99e">processing</a> is significantly more expensive than CSS
and its render-blocking execution can be arbitrarily long.</p>
<p>On top of that, non-blocking async JavaScript can also block rendering
in some cases, which we&#39;ll discuss further down.</p>
<p>At the same time, in many cases, JavaScript is the one responsible for
engaging user experiences on the web, so we cannot wean ourselves off it
completely.</p>
<p>What&#39;s the middle ground? How can we enable performant JS experiences?
Advice here actually varies, depending on the role of JavaScript in your
web app.</p>
<h3 id="js-enhanced-experience-aka-progressive-enhancement">JS Enhanced experience - AKA Progressive Enhancement</h3>
<p>Earlier we talked about the advantages of HTML as a
streaming format. This may be considered an old-school opinion, but in
order to create the highest performing web experience, it is often
better to build your application&#39;s foundations as HTML and (minimal)
CSS, and then later enhance it with JavaScript for improved user
experience.</p>
<p>That doesn&#39;t mean you have to shy away from fancy JavaScript based
animations or avoid dynamic updates to your content using JS. It just
means that if you can make your initial loading experience not
JavaScript-dependent, it is highly likely that it will be faster.</p>
<p>I&#39;m not going to go into details regarding writing progressively
enhanced web apps, as this is well documented elsewhere.</p>
<p>I&#39;m also not going to argue with the fact that in some cases, JavaScript
is mandatory and progressive enhancement makes little sense in your
case.</p>
<p>But if you&#39;re delivering content that the user then interacts with, it&#39;s 
likely better for you to deliver that content as
HTML, and then enhance it with JS.</p>
<h4 id="how-to-progressively-load-js">How to Progressively Load JS</h4>
<p>If you are following the progressive enhancement principles, you want to
load your JS in a non blocking way, but still want these enhancements to
be there relatively early on. More often than not, the web platform&#39;s
native mechanisms to load scripts will not be your friends. We talked in
length about blocking scripts and why they&#39;re bad, so you can probably
guess they are not the way to go. But what should you do?</p>
<h4 id="why-not-async">Why not Async</h4>
<p>First, let&#39;s talk about what you shouldn&#39;t do. <code>async</code> is an attribute
on the script element that enables it to be non-blocking, download in
parallel to other resources, and run whenever it arrives at the browser.</p>
<p>While that sounds great in theory, in practice it can result in race
conditions and can have
performance implications:</p>
<ul>
<li><code>async</code> scripts run whenever they arrive. That means they can run out of order so must not have any dependencies on any other script in the page.</li>
<li>Because they run whenever they arrive they can run either before or
after the page is first painted, depending on network conditions and
network optimizations. This creates a paradox: optimizing your async
scripts (better compress them or make sure they arrive earlier using
Server Push or preload) can often result in performance regressions!!
Arriving earlier to the browser means that their parsing and execution
(which on mobile can be hefty) is now <a href="https://calendar.perfplanet.com/2016/prefer-defer-over-async/">render blocking</a>, even though their
download was not!</li>
</ul>
<p>For that reason, <code>async</code> is not a great download mechanism, and should be
avoided in most cases.</p>
<p>One more thing: in order to even consider making certain scripts
<code>async</code>, those scripts must avoid using APIs such as <code>document.write</code>
which require blocking the parser at the point in which the script is
injected. They should also avoid assuming the DOM or the CSSOM are in a
specific state when they run. (so avoid e.g. appending nodes to the
bottom of the body element, or rely on certain styles to be applied)</p>
<h4 id="defer">Defer</h4>
<p>As far as native script mechanisms go, that leaves us with the <code>defer</code>
attribute. <code>defer</code> has the following characteristics:</p>
<ul>
<li>a deferred script will download without blocking the rendering of the page.</li>
<li>Deferred scripts run at a particular point in time, after the DOM
tree is complete and before the DOMContentLoaded event fires. That
means they run after all inline scripts, and can depend on them.</li>
<li>Deferred scripts execute in the order they are included in the
document, which makes it easier to have dependencies between them.</li>
</ul>
<p>So defer is a reasonable way to make sure scripts will not interfere
with the page&#39;s first render, but those scripts will delay the browser&#39;s
DOMContentLoaded event (which triggers JQuery&#39;s <code>ready()</code> callback). Depending on your app, that may be problematic if you&#39;re relying on user visible functionality to hang off of that event.</p>
<p>In terms of priorities, In Chromium both <code>async</code> and <code>defer</code> scripts are
(at the time of writing) being downloaded with low priority,
similarly to images. In other browsers, such as Safari and Firefox,
that&#39;s not necessarily the case, and deferred and async scripts have the
same priority as blocking scripts.</p>
<p>In order to defer scripts, similar limitations as to <code>async</code> apply: They
cannot include <code>document.write()</code> or rely on DOM/CSSOM state when they
run. (even though the latter is less restrictive, as there are
guarantees that they&#39;d run right before DOMContentLoaded and in order)</p>
<!-- TODO: make sure that Firefox is really not doing anything smart here -->

<aside>
Using `&lt;script defer>` only became a realistic option in the last few
years.
IE9 and below had a fatal issue with it, causing content that uses defer
to be [racily broken][ie9_defer], if the scripts actually have
interdependencies. Depending on the download order, deferred scripts
which add HTML to the DOM (e.g. using `innerHTML`) may trigger the
parser to start executing later scripts *before the first script has
finished running*. That was a huge blocker for the adoption of `defer`
scripts for many years.

<p>But since the usage of IE9 and older is very low nowadays, unless your
audience is very much old IE centric, it is probably safe for you to
use defer, and ignore potential issues.</p>
</aside>

<h4 id="why-not-blocking-js-at-the-bottom">Why not blocking JS at the bottom</h4>
<p>Sending your non-essential scripts as blocking scripts at the bottom of
the page used to be a popular mitigation technique to prevent blocking
scripts from slowing down first render. It was born in an age where
<code>defer</code> and <code>async</code> did not exist, and to be fair, was significantly
better than the alternative: blocking scripts at the top of the page.</p>
<p>With that said, it&#39;s not necessarily the best option, and has some downsides:</p>
<ul>
<li>While not as bad as blocking the HTML parser and DOM creation at the
top of the document, blocking scripts at the bottom of the page still
block them, which means DOMContentLoaded will be delayed.</li>
<li>Priority-wise blocking scripts at the bottom of the page are
downloaded with medium priority in Chromium based browsers, and with
high priority elsewhere.</li>
<li>Scripts at the bottom of the page are discovered late by the browser
(after the entire HTML is download and processed). Even in the advent
of the browser&#39;s preloader, their download can start relatively late.</li>
</ul>
<h4 id="dynamically-added-async-false">Dynamically added, async false</h4>
<p>Another method to dynamically load scripts is to insert them to
the document using JavaScript, but make sure they are loaded in order,
in case of dependencies between the scripts.</p>
<p>According to the HTML spec, when scripts are dynamically added to the
document, they are assumed to be async, so download starts immediately,
and they will execute whenever the are fully downloaded.</p>
<p><a href="https://www.html5rocks.com/en/tutorials/speed/script-loading/">Setting the <code>async</code> attribute to false</a> on such scripts changes their
behavior. They still don&#39;t block the HTML parser, and therefore do not
block rendering. At the same time, they will be executed in order.</p>
<p>The main advantage of this approach over previous mentioned ones, is
that the scripts will execute in order, similar to defer, but will not wait
till the DOM is fully constructed, right before DOMContentLoaded fires.</p>
<p>There are a few disadvantages though: This method requires a script in
order to load your scripts, which adds some cost. As a result of being
script based, these resources are not discoverable by the browser&#39;s
preloader, so their requests tend to kick off a bit later, and if the
page contains any blocking scripts above the loading scripts, they can
be delayed significantly. The advantage of the approach can also
become its disadvantage: the &quot;execute once you download&quot; approach can
result in premature execution, which can stall more critical code or
vital browser events (e.g. first paint).</p>
<h4 id="trigger-js-loads-only-after-the-first-raf-triggered-as-a-first-paint-proxy">Trigger JS loads only after the first RAF triggered, as a first paint proxy</h4>
<p>Like we&#39;ve seen for CSS, you can use <code>requestAnimationFrame</code> as a way to
load non-critical scripts after the first render happened, ensuring they
won&#39;t contend on bandwidth with critical CSS and JS, but still kicking
off their download as soon as possible.</p>
<h4 id="preload-with-onload-handler">Preload with onload handler</h4>
<p>Similarly, we can use preload and its <code>onload</code> event handler to kick off
script downloads and make sure they run once they finish downloading.</p>
<p>This is very similar to <code>async</code> script, with some subtle differences:</p>
<ul>
<li>Priority of preloaded scripts is assumed to be identical to that of
blocking scripts, while at least in some browsers, <code>async</code> will get
lower priority.</li>
<li>Unlike <code>async</code>, with preload you can make sure that the script doesn&#39;t
run until some milestone is hit (e.g. first paint happened)</li>
</ul>
<p>A simple version of this, mixing both preload and RAF, may look something like this:</p>
<pre><code>    &lt;script&gt;
        var firstPaintHappened = false;
        var scriptPreloaded = false;
        var addScriptElement = () =&gt; {
            var script = document.createElement(&quot;script&quot;);
            script.src = &quot;non-critical.js&quot;;
            document.body.appendChild(script);
        };
        var preloadOnloadHandler = () =&gt; {
            scriptPreloaded = true;
            if (firstPaintHappened) {
                 addScriptElement();
            }
        };
        if (window.requestAnimationFrame) {
            window.requestAnimationFrame(() =&gt; {
                firstPaintHappened = true;
                if (scriptPreloaded) {
                    window.setTimeout(addScriptElement, 0);
                }
            }
        } else {
            window.addEventListener(&quot;DOMContentLoaded&quot;, addScriptElement);
        }
    &lt;/script&gt;
    &lt;link rel=&quot;preload&quot; as=&quot;script&quot; href=&quot;non-critical.js&quot; onload=&quot;preloadOnloadHandler&quot;&gt;</code></pre><h4 id="missing-high-level-feature">Missing high level feature</h4>
<p>Similarly to CSS, preload gives us the platform primitives that enable
us to load script decoupled from their execution. But for CSS, we can
also now include styles in the body, and expect them to Just Work&trade;
without impacting the portion of the document that&#39;s above them.</p>
<p>For scripts, we don&#39;t currently have such a high level feature. An
attribute on the script element that will enable us to simply load them
without the script being blocking, while knowing that it will run,
potentially in order, at a certain milestone (first paint, after onload completed, etc).</p>
<p>This is a problem that&#39;s <a href="https://bugs.chromium.org/p/chromium/issues/detail?id=838761">being worked on</a> in Chrome,
where the problem is being defined and various solutions outlined.
Hopefully once that work is done, the solution will get standardized and
implemented in all other modern browsers.</p>
<h3 id="js-reliant-experience">JS reliant experience</h3>
<p>Previously we talked about web experiences that are augmented by JS:
where the content is built by HTML and CSS and JS is then used to
improve the experience.
That&#39;s not always the case on today&#39;s web. Many web sites today rely on
JS for their very basic rendering, adding those scripts to the web
site&#39;s critical rendering path.</p>
<p>That significantly delays the initial rendering of the page, especially
on mobile, as the browser needs to download hefty amount of scripts, parse them and execute them 
before it can even start creating the real DOM. That also means that
early discovery of resources, using the preloader, is not really
relevant in those cases, as the HTML tokens contain no interesting
information regarding the resources that will be required.</p>
<p>So, if you&#39;re starting out building a content site, I&#39;d suggest to avoid
building it in a way that relies on JavaScript for the basic rendering.</p>
<p>That doesn&#39;t necessarily mean that you cannot develop your site using
your favorite language and tools. Many frameworks today enable server side
rendering, where for the first page the user sees, the page gets
rendered with good old-fashioned HTML and CSS, and JavaScript kicks in
later, enabling the single page app experience from that point on. </p>
<p>Unfortunately, some popular JavaScript frameworks employ server-side
rendering as a way to get the content to the user early, but then
require large amount of JS execution in order to make that content
interactive. Whenever possible, you should steer away from such
frameworks and look for alternatives which server-rendered content is
functional on its own.</p>
<p>The best way to tell would be to test existing sites which use the
frameworks you&#39;re considering for your project and look at their
interactivity metrics on mobile. If the page takes a large number of
seconds to render on mobile, or if it renders quickly, but then frozen
in what seems like forever to the user, that&#39;s a big red flag indicating
you should avoid that framework.</p>
<p>You can run such tests, on real devices, at <a href="https://www.webpagetest.org/">webpagetest.org</a>. You
could also run a <a href="https://developers.google.com/web/tools/lighthouse/">lighthouse</a> <a href="https://www.webpagetest.org/lighthouse">audit</a> of those sites,
pointing out the site&#39;s Time to Interactive metrics, among others.</p>
<h4 id="but-i-already-do-rely-on-js-what-do-i-do-">But I already do rely on JS. What do I do?</h4>
<p>In case you already rely on JS for your existing site, look into
server side rendering solutions. If they exist for your framework, they
will help speed up your initial rendering. Unfortunately SSR solutions can&#39;t always
help your site&#39;s interactivity, if they weren&#39;t designed with that in
mind. That is because they often involve “hydrating” the DOM - attaching
event listeners and binding data structures to it, which can require
significant processing.</p>
<p>If there&#39;s no server-side rendering solution for your case, <code>&lt;link rel=preload&gt;</code> can help you overcome the fact that your site
is sidestepping the browser&#39;s preloader, and give your browser&#39;s network
stack something to do while the user&#39;s CPU is churning away executing
JavaScript. Because Priority Hints are not yet widely implemented, you
need to make sure these preload links are either included in the HTML
after more critical resources, or added to it dynamically after first
paint.</p>
<p>You may be able to switch to a lighter-weight version of your
framework. Many popular frameworks have a lighter alternative, which is
while being faster. There are of-course some trade-offs to be had there,
as that lighter weight may come at a cost to functionality you rely on.
But in many cases, it just sheds off features you don&#39;t even use,
resulting in faster experience at no particular cost. </p>
<p>Another way to make your JS reliant app faster is to use intelligent
bundling solutions like WebPack, Rollup or Parcel to code-split your
application into route-specific bundles, and only load upfront the code
you need for your current route. Then you can use prefetch to download
future routes as lower priority resources</p>
<p>Eventually, consider rewriting the major landing pages for your
application, so that users can get them rendered and working relatively
fast, and then use them to &quot;bootstrap&quot; the rest of your app.</p>
<!-- TODO: passing that through Andy and Kadlec -->

<h2 id="image-lazy-loading">Image lazy loading</h2>
<p>Images often comprise a large chunk of the page&#39;s downloaded bytes. At
the same time, many of the images on the web are downloaded, decoded and
rendered only for the user to never actually see them, as they were too
far off the initial viewport, and the user never scrolled that far.</p>
<p>Lazy loading out-of-viewport images is something that JS libraries have
been experimenting with for a while.
There are many decent open source solutions for you to pick from, and
they can significantly reduce the amount of data your users download by
default, and make their experience faster.</p>
<p>A few things to note regarding lazy loading solutions:</p>
<ul>
<li>Lazy loading your in-viewport images is likely to make them slower, as
they will be discovered by the browser later.</li>
<li>The amount of in-viewport images may vary widely between different
viewports, depending on your application.</li>
<li>You need to make sure that when the images do get downloaded, that
doesn&#39;t cause a relayout of the entire content, as that will introduce
jank to the scrolling process.</li>
</ul>
<p>The first two points are hard to get right, as they mean you would need
to automatically generate your HTML based on the device&#39;s viewport
dimensions, to include the in-viewport images in markup and lazy load
the rest. The third requires you to pay particular attention to your
markup and styles, and make sure your images are well contained in terms
of its width as well as its height.</p>
<p>Recently browsers have <a href="https://groups.google.com/a/chromium.org/forum/#!msg/blink-dev/czmmZUd4Vww/1-H6j-zdAwAJ">started looking</a> into doing that kind of lazy
loading natively, but the same problems apply there. Currently the
browser doesn&#39;t know which images will be in the viewport, nor what
their dimensions before at least some parts of the image are downloaded.
I believe we will need to find a standard way for developers to communicate that
information to the browser, so it can do the right thing. It will also
be helpful to define a simple way for developers to define an element&#39;s
aspect-ratio, rather than rely on <a href="https://css-tricks.com/aspect-ratio-boxes/"><code>padding-top</code> based hacks</a>.</p>
<h3 id="intersectionobserver">IntersectionObserver</h3>
<p>JavaScript based lazy loading have become significantly easier to
implement in a performant way with the introduction of
<a href="https://developers.google.com/web/updates/2016/04/intersectionobserver">IntersectionObserver</a>. 
Traditionally, JS based lazy loading was based on listening to the
browser&#39;s scroll events, and calculate the image&#39;s position relative to
the viewport on each one of those events. In many cases, that resulted in a lot of
main-thread jank, as it needed to perform these calculations as the user
was scrolling. IntersectionObserver enables developers to get a callback
whenever a certain element is in a certain distance from the viewport.
That enables lazy loading implementations achieve their goal without
spamming the main-thread&#39;s event loop.</p>
<h3 id="placeholders">Placeholders</h3>
<p>You can also include a low quality placeholder to replace your image
until it gets loaded. This has a couple of advantages:</p>
<ul>
<li>In case your users scrolled fast and the image has not loaded yet, a
placeholder gives them a better experience.</li>
<li>It will resolve the re-layout issues without you having to define
explicit image dimensions or CSS-based aspect ratios.</li>
</ul>
<p>In its simplest form, you could create a thumbnail of your image and
incorporate it in your markup as a data URI. You&#39;re probably better off
doing that using some build-time automated solution rather than
manually.</p>
<p>There are also <a href="https://github.com/technopagan/sqip">more advanced techniques</a> available that can help
you get a nicer blurry image, without paying more in terms of bytes.</p>
<h3 id="what-to-do-">What to do?</h3>
<p>A few points worth looking into when picking an image lazy loading
solution:</p>
<ul>
<li>It should get bonus points for using IntersectionObserver, as it&#39;ll be
likely to be less-janky and more accurate.</li>
<li>You should avoid applying lazy-loading for images that are in the
viewport for most reasonable viewports your users use.</li>
<li>Make sure your images&#39; dimensions are well defined in markup or
styles. Alternatively, include an inline placeholder,
which will maintain the layout as well as present something to your
users while they are waiting for the full image to load.</li>
</ul>
<h2 id="fonts">Fonts</h2>
<p>When classifying resources to rendering-critical vs non-critical ones,
fonts sit somewhere in the middle. On the one hand, fonts do not block
the rendering of the page’s general layout. On the other, in many cases,
they do block the page’s headers or text from becoming visible,
preventing the user from seeing the content.</p>
<p>The default behavior for font loading varies between different browsers,
where IE and Edge show the user a fallback font while they wait for the
fonts to download, while others prefer to block font rendering for 3
seconds, waiting for fonts to arrive, and only then render the fallback
font. So you could consider the default font loading behavior of most
browsers to be blocking, or at least “blocking for 3 seconds”, which is
not ideal.</p>
<p>So, what’s the best way to control your font loading and make sure your
users don’t find themselves staring for seconds at a fully laid-out page
with no text to read?</p>
<p>There are <a href="https://www.zachleat.com/web/comprehensive-webfonts/">various strategies</a> you can take to tackle this, but you have
two major options.</p>
<h3 id="font-display-css-rule">Font-display CSS rule</h3>
<p>The <code>font-display</code> CSS rules are fairly widely supported (all modern
browsers except for Edge), and can enable you to control the browser’s
behavior regarding font loading and modify it to match your preferences:
If you can live with the Flash of Unstyled Text (FOUT), where the
fallback fonts are rendered first, and then replaced by the loaded fonts
once they arrive, <code>font-display: swap</code> is probably the way to go.</p>
<p>Alternatively, <code>font-display: fallback</code> enables you to make sure that
a switch from fallback font to loaded font won’t happen too late in the
page load, and guarantees that if such a switch happens, it will only happen within the first 3 seconds
of the font loading.</p>
<p>If you can’t live with FOUT, <code>font-display: optional</code> makes sure that
the loaded fonts are displayed only when they are immediately available
to the browser (e.g. when they are cached from a previous session).
Otherwise, only the fallback font is displayed.
Since Edge’s default font loading strategy is not very different from
“swap”, the lack of support is probably not something you have to worry
about too much.</p>
<p>One major <a href="https://www.zachleat.com/web/font-display-reflow/">caveat</a> with CSS font-display is that different fonts may
trigger the swap at different times, so if you have multiple font files
that represent different styles or weights, they may come in at
different times, resulting in a potentially jarring experience to your
users.</p>
<p>If this is your case, you may prefer to use…</p>
<h3 id="the-font-loading-api">The Font Loading API</h3>
<p>The Font Loading API enables you to explicitly load fonts from JS, and
perform certain actions when they finished loading. As such, if you have
several font files which need to be rendered together, you can load
programmatically, and change the CSS rules to apply them (e.g. by adding
a class on their container) only once all of them finished loading.</p>
<h1 id="full-bandwidth-pipe">Full Bandwidth Pipe</h1>
<p>Because web content is comprised of many smaller resources,
traditionally, it has been difficult to make sure the network&#39;s
bandwidth is well-utilized when delivering it. We discussed early
delivery earlier, but that&#39;s not the only piece for that puzzle.
With HTTP/1, requests often incurred the overhead of starting their own
connections, which introduced a delay between the time the resource was
discovered to the time it was requested.
That delay still exists, to a lesser extent, for HTTP/2 for third party content. A separate connection still needs to be established.</p>
<p>Resource discovery itself is another tricky piece. Since web content is
progressively discovered, it means that network efficiency is often
blocked on processing efficiency (the browser has to do work on
previously loaded resources in order to discover and request future
ones).
It also means that resource loading is latency bound as at least a full
RTT has to be spent on downloading and processing each layer of the
loading dependency tree, which is required to discover and download the
next one.</p>
<h2 id="early-discovery">Early Discovery</h2>
<p>What can we do in order to speed this process up and avoid this process
being latency and processing bound?</p>
<p>H2 Push, which we discussed earlier, is one way to make sure that by the
time critical resources are discovered by the browser, they are already
downloaded and are safely stowed in its cache.</p>
<h3 id="preload">Preload</h3>
<p>Another, slightly more flexible alternative is preload.</p>
<p>Preload enables us to declaratively fetch resources ahead of time, in a
way that is decoupled from their usage. That means that for
predictable late-discovered resources, we could include preload links for them in
the document, and let the browser know about them and fetch them early on.
Preloaded resources are downloaded using the right priorities for their
resource type, which is defined using the <code>as</code> attribute.</p>
<p>Preload&#39;s <code>as</code> attribute enable the browser to know what resource type
it is fetching and therefore enables it to download it with the right
priority, while using the correct <code>Accept</code> headers.</p>
<p>Using it can be as simple as including 
<code>&lt;link rel=&quot;preload&quot; href=&quot;late_discovered_thing.js&quot; as=&quot;script&quot;&gt;</code> in
your markup.</p>
<p>You can also use preload&#39;s <code>onload</code> event to create more sophisticated
loading patterns, as we&#39;ve seen previously.</p>
<p>One thing to note when using preload is the <code>crossorigin</code> attribute.
The HTMLLinkElement’s default credentials mode is &quot;include&quot;. Therefore, when
preloading resources with a different credentials mode (e.g. fonts, as well
as <code>fetch()</code>, <code>XMLHTTPRequest</code> and ES6 modules by default), you need to
make sure the <code>crossorigin</code> attribute is properly set on your preload
link, otherwise the resource may not be reused (since the internal
caches will refuse to serve the preloaded response to the future request) which may result in
double downloads.</p>
<h3 id="preloads-jumping-the-queue">Preloads &quot;jumping the queue&quot;</h3>
<p>While working on preload, it was considered a panacea that will enable
us to &quot;PRELOAD ALL THE THINGS!&quot; and solve the discovery problem in web
content.
However, it turned out that things are not that simple.
One concern with using preloads is that early discovery also means that
we rely more heavily on the server to prioritize the content according
to HTTP/2 priorities, and avoid blocking high-priority content on
low-priority one.
And since in HTTP/2 the browser has no request queue, discovered resources
are immediately sent to the server with their appropriate HTTP/2
priority.</p>
<p>HTTP/2 priorities are fairly complex and not all servers fully respect them.
Also, because HTTP/2 is built over TCP, prioritization is even
trickier. It&#39;s possible that the server would start sending
low priority resources, then switch to high-priority ones, but have the
low-priority resources fill up the TCP queues, blocking more critical
content.</p>
<p>That means that the order of requests in HTTP/2 matters as well as their
priorities if not sometimes more. As a result you should be careful that 
using preload does not result in low-priority resources being requested
before high-priority ones.
You can do that by incorporating the <code>&lt;link&gt;</code> elements in your markup
below the critical-path resources. For <code>Link</code> headers, there&#39;s <a href="https://chromium-review.googlesource.com/c/chromium/src/+/720798">work underway</a> to resolve that in Chromium.</p>
<p>There are also issues when preloaded content is delivered over separate
connections. In those cases, there is no server to correlate the
requests according to their priorities. But we&#39;ll discuss that more when
we talk about contention.</p>
<!-- TODO: do we need to add example waterfalls here? -->

<h1 id="minimal-content">Minimal content</h1>
<p>We talked earlier about properly splitting and prioritizing critical and
non-critical content, but turns out that on the web there a third class
of content: unneeded content.</p>
<h2 id="don-t-download-unused-content">Don&#39;t download unused content</h2>
<p>Due to the use of CSS frameworks, large JS libraries, as well as simple code churn, content on the web tends of contain a large percentage of unused code.
There are many tools today that can help you spot such unused code
as part of your build process and weed it out.</p>
<p><a href="https://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md#class-coverage">Puppeteer coverage API</a> enables you to detect such unused
content and take action on it. It is very easy to use such tools to see
how much unused CSS and JS you have on your page. It may not be as
trivial to take action on those unused parts and delete them
automatically, and I&#39;m not aware of current tools which do that.
But at the very least you should be able to use these tools to monitor
the amount of unused JS in your application.</p>
<p>With that said, it&#39;s a bit tricky to use such tools to distinguish unused code from code that will be used later on in
some user scenario. That&#39;s probably the part that requires developer
intervention and understanding of your application.
But once you detect such code, it would probably be better to lazy
load it when that scenario is likely to actually happen, rather
than loading it upfront and penalize users for no reason.</p>
<p>One caveat with such tools that use headless Chrome for unused code
detection - they will declare polyfills for features implemented in
Chrome and not implemented elsewhere as &quot;unused code&quot;. While that is
technically true, that code is likely to still be needed in non-Chromium
browsers. That is something worth keeping in mind when removing unused
code.</p>
<p>As always, it&#39;s significantly easier to make sure you don&#39;t get unused
code into your site when you build it than it is to weed out unused code
once you realize you have lots of it on your site. Finding out when each
piece of code comes from and where it might be used can be a tiresome
manual process. So avoid getting yourself into such scenarios by having
proper coverage checks as part of your continuous integration and
deployment process. That helps to make sure that whenever you
incorporate a new library for that shiny new feature, you won&#39;t incorporate unneeded bloat along with it.</p>
<h3 id="but-that-unused-content-is-cached-so-that-s-perfectly-fine-right-">But that unused content is cached, so that&#39;s perfectly fine, right?</h3>
<p>Well, not really. First of all, that content will not be cached for
first visit users. Depending on your audience, that could be a large
chunk of your users, and as they say, you only get one opportunity to
make a first impression. That content will also not be cached whenever
you update it, which is you should do fairly regularly, to avoid any
<a href="https://snyk.io/blog/77-percent-of-sites-still-vulnerable/">known security vulnerabilities</a> in popular libraries.</p>
<p>On top of that, caches get evicted. Especially on mobile devices (where it
matters most), unless your site is the most popular one that user
visits, your precious resources may have made room for others. Having
overly bloated resources (which take up more cache space) actually
increases the chances of that happening. And even if the resource is in
the cache, larger resources are more likely to be stored over multiple
blocks, which on spinning-disk based caches may mean longer retrieval
times.</p>
<p>For JavaScript, extra code also means extra parsing costs. Some
JavaScript engines cache their parsing products, making processing
faster in repeat views. For example, V8 uses <a href="https://v8project.blogspot.com/2018/04/improved-code-caching.html">code
caching</a> to make sure
JS bytecode can be reused in repeat views, reducing parsing costs there. 
But that may not be true for all JS engines and
all JS content (as some content may not be eligible for caching). There
are also no guarantees that the cached parsing products won’t get
evicted before your content does.</p>
<!-- TODO: Make sure that's actually true -->

<p>Finally, unused CSS and JS code increases your site&#39;s memory footprint
for no good reason, as the browser has to maintain the unused rules and
code in memory for as long as your site is in memory. In low-memory
environments (e.g. mobile) that could mean the difference between your
site&#39;s tab getting kicked out of memory when the user is not looking.</p>
<h2 id="compression-api">Compression API</h2>
<p>As mentioned above, in many cases the unused content on our sites is
there due to inclusion of frameworks, where we don&#39;t use every bit of
functionality in them. One solution for that could be to compress that
data away.</p>
<p>How can we do that? If we look at framework data on the web, we&#39;ll see
that a lot of it is shared across sites and can theoretically be very
efficiently compressed if we were to use a static dictionary that is
based on some older version of that framework.</p>
<p>Gzip have always had static compression dictionaries (albeit limited in
size). Brotli recently joined it and defined <a href="https://tools.ietf.org/html/draft-vandevenne-shared-brotli-format-00">shared brotli</a> dictionaries.</p>
<p>At the same time, there&#39;s been <a href="https://discourse.wicg.io/t/a-zip-api-in-the-browser/14/26">proposals</a> to create a browser native compression API,
which will enable developers to make use of such dictionaries in order
to decompress data.</p>
<p>Combining these two efforts will enable us to use compression to get rid
of most of our unused code and not transport it to our users every time.</p>
<p>Two things are worth noting regarding that:</p>
<ul>
<li>These efforts are both still in their early phases, so it may take a
while before you can take advantage of them.</li>
<li>Compressing the bytes over the network doesn&#39;t prevent us from
paying up their parsing costs at runtime, as the parse time of JS is
related to its uncompressed size. So even if we could use it
today, this should only be a fallback in cases where the removal of
unused code is not feasible.</li>
</ul>
<h2 id="web-packaging">Web Packaging</h2>
<p><a href="https://github.com/WICG/webpackage">Web Packaging</a> is a new standard proposal for a bundling
format - a format that will enable us to send multiple different
resources in a single, well, package.</p>
<p>The primary use case for that format is to be able to bundle pieces of
content and enable users to pass them along between themselves (without
going through a server) or enable content aggregators (such as Google
Search) to serve such content from their servers, while the browser
still considers the content provider&#39;s origin as the real origin (from a
security perspective, as well as from a browser UI one).</p>
<p>But a secondary use for such a bundling format could be to improve the
web&#39;s bundling capabilities.</p>
<p>Historically, web developers used to concatenate their scripts and
styles in order to reduce the number of requests passing through, as
with HTTP/1.1, each request incurred a cost. With the move to HTTP/2,
the promise was that bundling and concatenation will no longer be
necessary.</p>
<p>But in practice, developers soon <a href="https://engineering.khanacademy.org/posts/js-packaging-http2.htm">found out</a> that
bundling still has a role:</p>
<ul>
<li>HTTP/2 does not have cross-stream compression contexts (for security
reasons), which means that each resource is compressed on its own. But
small files compress with a significantly lower ratio than large ones.
So if you have many small JS files in your application, you&#39;ll get much
larger overall JS payload over the network if you would bundle your
small files into a single one.</li>
<li>Even though HTTP/2 has no extra cost per request over the network,
that doesn&#39;t mean that requests have no extra overhead in the browser.
At least in Chrome, that overhead can make a difference when added up
among hundreds of requests.</li>
</ul>
<p>So, bundling still has a role and can overall improve performance if
your site has many different JS or CSS resources. But, currently,
bundling also has a cost. We&#39;ve talked earlier about the fact that both
JavaScript and CSS are resources that must be executed in their
entirety. When we bundle resources together, we effectively tell the
browser that they cannot be executed separately, and none of them starts
executing until all of them were downloaded and parsed.</p>
<aside>
Chrome 66 introduced a [streaming parser][stream_parsing] for JavaScript so it can be
parsed on a background thread as it is being downloaded.
That means that at
least in some cases, the "all or nothing" approach to JS is slightly
mitigated. So even though large bundles still have to be executed as a
single unit, they may be parsed progressively.
Other browsers don’t yet have streamed parsing for JS, but at least some
of them are actively looking into that area.
</aside>

<p>Another downside of bundling is the loss of caching granularity. When
serving resources separately, if any of them is no longer fresh and
needs updating, they can be downloaded on their own, and all the rest can be retrieved from cache.
But once we&#39;ve bundled resources, each small change in each one
of the files means that all of them must be downloaded again. Even
worse, for JavaScript, it means that the whatever parsing products that
the browser created and cached for that file is no longer valid, and the browser
has to create it again, spending precious user CPU time on that.</p>
<p>The current advice today for developers is to find the right trade-off
between those two approaches: bundle scripts, but avoid creating huge
bundles which will delay the scripts&#39; execution and increase the
probability of unnecessary cache invalidation.</p>
<p>But let&#39;s go back to WebPackaging and how it can help us here. It can
enable us to create a bundle comprised of many smaller files, with each
one of them maintaining its identity as an independent entity. That
means each one of them can be cached separately (based on its own
caching lifetime directives) and processed separately. Therefore, we
would no longer have to download the entire package in order to update
e.g. the first resource in it, and we won&#39;t have to invalidate all of it
if a single resource in it has changed or needs revalidation.</p>
<p>At the same time, we would be able to compress the entire bundle as a
single resource, providing us the same compression benefits as today&#39;s
bundling would.</p>
<p>This is all in its early days, and there&#39;s currently no concrete
proposal to enable this, but it&#39;s seems that almost all the building blocks for
this are in place or in motion, so hopefully this will become a reality
sooner rather than later.</p>
<h2 id="images">Images</h2>
<p>Images is another place where we can easily send our users
content bytes which they don&#39;t need and use - either by sending
oversized images or by sending them under-compressed.</p>
<p>This subject is way too-wide to be fully covered in this section. It has
been covered by an entire chapter in <a href="https://shop.smashingmagazine.com/products/smashing-book-5-real-life-responsive-web-design">Smashing Book 5</a>,
as well as an <a href="https://www.akamai.com/us/en/multimedia/documents/content/e-book/akamai-oreilly-high-performance-images-e-book.pdf">entire book</a> dedicated just for image optimization.</p>
<p>But I&#39;ll do my best to give a brief summary.</p>
<h3 id="responsive-images">Responsive Images</h3>
<p>The responsive images solutions goal is to enable developers to serve
appropriately dimensioned images to their users, no matter their device
dimensions and how big the images are on that device.</p>
<p>Using the responsive images
solutions, and in particular <code>srcset</code> and <code>sizes</code>
enables you to provide the browser with multiple alternative image
sources and let the browser pick the one that&#39;s most fitting for the
user&#39;s device.</p>
<p>The <code>srcset</code> attribute declares multiple image alternatives to the
browser, as well as their descriptors. <code>x</code> descriptors tell the browser
about the image resource&#39;s density. <code>w</code> descriptors tell it about
the image resource&#39;s width.</p>
<p>These enable the browser to pick the right resource, based on the
device&#39;s screen density as well as its viewport dimensions.</p>
<p>Using <code>x</code> descriptors to serve a fixed width image may look something like:</p>
<pre><code>    &lt;img src=&quot;dec_500px.jpg&quot;
        srcset=&quot;dec_750px.jpg 1.5x, dec_1000px.jpg 2x, dec_1500px.jpg 3x&quot;
        width=&quot;500&quot; alt=&quot;The December wallpaper&quot;&gt;</code></pre><p>Using <code>w</code> descriptors to serve an image which can be in many different dimensions:</p>
<pre><code>    &lt;img src=&quot;panda_fallback.jpg&quot;
         srcset=&quot;panda_360.jpg 360w, panda_540.jpg 540w, panda_720.jpg 720w, 
                 panda_1080.jpg 1080w, panda_2160.jpg 2160w, panda_3240.jpg 3240w&quot; 
         alt=&quot;A panda eating some bamboo.&quot;&gt;</code></pre><p>The <code>sizes</code> attribute enables us to make the matching even tighter. When
the browser is requesting the images, it still isn&#39;t aware of what their
display dimensions would be (as layout may not have happened yet).
<code>sizes</code> enables us to tell the browser what the image display dimensions
would be in different breakpoints of the design, so the browser can
calculate the ideal resource provided by <code>srcset</code> and its <code>w</code> descriptors.</p>
<p>The sizes value could be relatively simple, just stating a percentage of
the viewport, such as:</p>
<pre><code>    &lt;img src=&quot;tiger_500px.jpg&quot;
        sizes=&quot;33vw&quot;
        srcset=&quot;tiger_200px.jpg 200w, tiger_500px.jpg 500w, tiger_800px.jpg 800w, 
                tiger_1000px.jpg 1000w, tiger_1500px.jpg 1500w, tiger_2000px.jpg 2000w&quot;
        alt=&quot;Tiger&quot;&gt;</code></pre><p>It can also become a bit more complex in more involved designs:</p>
<pre><code>    &lt;img src=&quot;thumb.jpg&quot; 
         sizes=&quot;(min-width: 1200px) 235px,
                (min-width: 641px) 24vw,
                (min-width: 470px) 235px,
                50vw&quot;
         srcset=&quot;thumb100.jpg 100w, 
                 thumb200.jpg 200w, thumb235.jpg 235w,  
                 thumb300.jpg 300w, thumb470.jpg 470w&quot;
         alt=&quot;A rad animal.&quot;&gt;</code></pre><h4 id="client-hints">Client Hints</h4>
<p>Client Hints is a content negotiation mechanism which can be also be
used to serve responsive images, among other resource types.</p>
<p>Content negotiation means that the client indicates to the server various
parameters about the resources that it is interested in, and the server
uses that in order to send it with the right resource.</p>
<p>With Client-Hints, once the server has opted-in (using <code>Accept-CH</code> and
<code>Accept-CH-Lifetime</code> headers), the browser can start sending the server
information regarding its <code>DPR</code> (device pixel ratio, or screen density),
<code>Viewport-Width</code> as well as the image&#39;s <code>Width</code> (if the image has a
<code>sizes</code> attribute attached to it). It can also indicate whether the user
has opted-in to data savings mode, using the <code>Save-Data</code> client hint
header.</p>
<p>That enables the server to serve the right resource to the client,
without requiring markup changes (or while requiring smaller ones).</p>
<h3 id="image-formats">Image formats</h3>
<p>Another aspect of image optimization is properly compressing them. And
the amount of gain we can get from image compression varies greatly
based on the image format we pick.</p>
<p>Different browsers support different cutting-edge image formats, which
means that serving the right image format in all cases requires some
amount of sophistication.</p>
<p>Serving different image formats to different browsers can be done in a
couple of ways.</p>
<p>If you have control over your server, it&#39;s probably easier for you to
use the image request <code>Accept</code> headers in order to detect if the browser
supports WebP, JPEG-XR or just the older (but reliable) JPEG and PNG.
Then you can use your server-side logic to pick the ideal resource for
that particular browser.</p>
<p>If you have no server-side control, you can use another variant of the
responsive images markup to let the browser pick which format it
supports.</p>
<pre><code>    &lt;picture&gt;
        &lt;source type=&quot;image/webp&quot; srcset=&quot;president.webp&quot;&gt;
        &lt;source type=&quot;image/vnd.ms-photo&quot; srcset=&quot;president.jpxr&quot;&gt;
        &lt;img src=&quot;president.jpg&quot; alt=&quot;The president fistbumps someone.&quot;&gt;
    &lt;/picture&gt;</code></pre><h2 id="font-subsetting">Font subsetting</h2>
<p>Web fonts is another type of content where you can send your users
excessive data. Fonts often contain full set of characters which fall
outside of the basic Latin Unicode block (typically used in
non-English languages), which may or may not be relevant for your
content. These extra characters can add up, and if you&#39;re not using
them, it might be best to drop them from the downloaded font entirely.
At the same time, it&#39;s tricky to assume you what you will and will not
be using, as e.g. names and locations may contain them. Those can also
be casually added to your site by content creators (or your users - for
user generated content) after you have subsetted the fonts.</p>
<p>In cases where you can make assumptions about the content at build time,
there are tools (e.g. [subfont][subfont] or [glyphhanger][glyphhanger]) which can help you subset your
fonts to the minimal subset you need.
[subfont]: <a href="https://www.npmjs.com/package/subfont">https://www.npmjs.com/package/subfont</a>
[glyphhanger]: <a href="https://github.com/filamentgroup/glyphhanger">https://github.com/filamentgroup/glyphhanger</a></p>
<h1 id="contention-avoidance">Contention avoidance</h1>
<p>We talked earlier about resource priorities and the way that browsers
handle priorities in HTTP/2 by sending the requests to the server, and
letting it send the high priority resources first. However, there’s one
big problem in that scheme: on the web today, <strong>there is no single
server</strong>. Most sites are served from a multitude of servers, as
different first-party resources are often served from different hosts
(e.g. static files served from static hosting providers, images served
from an image optimization service, etc).  On top of that, 3rd party
resources are served from servers outside of the publisher’s control.</p>
<p>Each of these servers has no visibility into the other resources
downloaded from other hosts, and therefore cannot take them into account
when considering resource priorities, as it can only prioritize the
resources that it is sending the user.</p>
<p>In practice, that often means that every server handles only a handful
of resources, in many cases of similar priority. So the server
frequently find itself with a low-priority resource at the top of the
priority queue, and therefore sends it to the user, as it is the highest
priority resource on the connection!</p>
<p>That often leads to bandwidth contention between critical resources on
the page and less-critical ones that are served from different hosts.</p>
<h2 id="connection-pools">Connection pools</h2>
<p>Another source of contention is the fact that at least some browsers
open different connection for non-credentialed resources. That mostly
impacts fonts, XHR and <code>fetch()</code> based resources, but it means that
these resources are fetched on a separate connection where the same
problem as above applies - these resources are fetched without taking
into account most of the critical resources on the page&#39;s &quot;main&quot;
connection - the credentialed connection.
Firefox has <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1363284">changed that behavior</a> starting from Firefox 60, but
Chrome still uses separate connection pools.</p>
<h2 id="http-2-connection-coalescing">HTTP/2 Connection Coalescing</h2>
<p>HTTP/2 has a mechanism that enables you to get rid of some of these
duplicate connections: If multiple hosts on your site all map to the
same IP and are all covered by a single certificate (so cross-origin
hosts are covered by the navigation connection&#39;s certificate
Server-Alternate-Name, or SAN, extension), then the browser is able to coalesce
all those connections together on a single host.
While that is a great way to minimize the number of connections, it
comes with some hurdles:</p>
<ul>
<li>A DNS request is still required (to verify that these hosts map to a
single IP), adding some latency to the process.</li>
<li>It&#39;s not always easy to convince your IT department to add multiple
hosts as your cert&#39;s SAN.</li>
<li>DNS based load balancing can mean that it&#39;s tricky to make sure these
hosts always map to the same IPs, and different browsers do different
things when the hosts map to multiple IPs.</li>
</ul>
<p>The fact that <a href="https://daniel.haxx.se/blog/2016/08/18/http2-connection-coalescing/">different browsers implement slightly different DNS
requirements</a> makes using this mechanism
even less reliable in practice.</p>
<h2 id="secondary-certs">Secondary certs</h2>
<p>One solution to make connection coalescing easier is an upcoming proposal called
Secondary Certs, which will enable some of those unrelated servers to
share a single connection and properly prioritize the content on it.
It enables the navigation connection to declare its authoritativeness
over the other connections used on the site.</p>
<p>A connection would prove authoritativeness over other hosts
by letting the browser know that it holds the private keys for their
certificates. That process is, in a sense, similar to a TLS handshake,
but is performed on the same initial connection. After the server has
proved it can handle requests for those other hosts, the browser can
simply send those requests on that same connection, avoiding connection
establishment and slow-start overhead, and most of all, avoid bandwidth
contention, as the server can handle the priorities of requests from
different hosts on a single connection, and as part of a single priority
queue.</p>
<h2 id="delayed-requests">Delayed requests</h2>
<p>Another potential solution to the bandwidth contention problem is to
bring back client-side request queues to HTTP/2, and make sure that
low-priority requests which will go on a different connection than the
&quot;main&quot; one, will get buffered on the client side until some point where
we are certain they will not slow down the more critical resources.</p>
<p>While doing that kinda works for HTTP/1, for HTTP/2 it seems like we can
so better.
Delaying the requests means that they will hit the servers half an RTT
after we decide it&#39;s OK to send them because the risk for contention is
gone (e.g. the critical resources have downloaded). Also, by default, if
we hold back the requests, no connection will be established. We could
in those cases implicitly preconnect to those hosts.
That&#39;d be better, but
maybe still not perfect.</p>
<h2 id="net-stack-orchestration">Net stack orchestration</h2>
<p>A better alternative would be to be able to send those requests to the
related servers, but use HTTP/2&#39;s session and stream flow controls in
order to make sure the responses are not sent back (or don&#39;t take more
than a certain amount of bandwidth) until the critical resources are
well on their way.</p>
<p>If done well (with some heuristics, since we&#39;re not certain we know the
size of the critical resources coming in), that can lead to perfect
network stack based orchestration of content coming in from multiple
connections.</p>
<h2 id="same-connection-h2-contention">Same-connection H2 contention</h2>
<p>The previous section all discussed contention in an H2 world where there
are multiple connections, and resources on these separate connections
are contending with each other. But there are also scenarios in H2 where
resources contend with same priority resources on that same connection.</p>
<p>Consider a scenario where multiple JavaScript resources are being
downloaded from a server, all in the same priority. The server can send
those resources down in 2 different ways:
1) Interleave the resources, sending a buffer of each of them.
2) Send the resources one after the other.</p>
<p>Which of these approaches would be a better one?</p>
<p>When talking about JS or CSS resources, the answer is obvious. Since
these resources need to be processed in their entirety before they can
be executed, sending the full files would mean we can execute each of
those files earlier, resulting in faster overall experience.</p>
<p><img src="media/parallel_vs_1by1.jpg" alt=""></p>
<p>However, what if we&#39;re talking about resources that are processed in a
streaming fashion, such as images? In that case, it&#39;s probably better to
send down some data for each separate image, and provide the user with
better visual feedback. That is particularly true for progressive JPEGs,
where sending the first scan of each of the viewport images is enough to
give the user the impression that the screen is full, even if the images
are still of low quality.</p>
<p>Browsers can control the way that H2 servers send their resources and
try to influence them, using H2&#39;s dependencies and weights. While
weights represent the resource&#39;s priority, dependencies can tell the
server to prefer to send a resource only after the entire resource is
depends on was sent down. In practice, that translates to the server
sending resources on-by-one, if it&#39;s possible.</p>
<p>The notion of in-stream H2 priority, discussed earlier, would enable browsers to indicate servers that e.g. a certain resource&#39;s first few chunks are more important than its end. </p>
<h2 id="what-can-you-do-about-it-">What can you do about it?</h2>
<p>Unfortunately, most of the items listed above are things that browsers
would need to improve in order for you to take advantage of.</p>
<p>But there are still a few things you can do in order to minimize the
negative impact of third party downloads on your main resources.</p>
<h3 id="rewrite-static-third-party-resources-to-your-own-domain-">Rewrite static third party resources to your own domain.</h3>
<p>If at all possible, you&#39;d be better off rewriting static third party
resources to point to your own domain, and re-host them there. That
would mean that from the browser&#39;s perspective, these resources are now
first party, and can hitch a ride on the existing first-party H2
connection, and not contend on bandwidth with your own resources.
Unfortunately, you have to be pretty sure these resources are public, static and
not personalized in any way. From a security perspective, these
resources can now be inspected by code running on your site, so you need
to make sure these third party resources are public resources, to avoid
any security issues.</p>
<p>Another reason to make sure these resources are public is that if that resources depend on cookies in any way,
these cookies will be lost when rewriting the URLs.</p>
<h3 id="delay-requesting-of-non-critical-third-parties-">Delay requesting of non-critical third parties.</h3>
<p>Another alternative to avoid contention is to time-shift the
non-critical third party requests to a point in time where they will not
contend with your main content. That may look very much like the
lazy-loading of non-critical JS content, but potentially delayed even
further, to avoid contention with your own non-critical content.</p>
<h3 id="use-connection-coalescing-to-your-advantage-">Use connection coalescing to your advantage.</h3>
<p>We talked earlier about H2 connection coalescing and all of its caveats. But even with all those downsides, HTTP/2 connection coalescing is the only
mechanism you can use today in order to reduce the proliferation of
different connections competing with each other on your site. Connection
coalescing is something you can take advantage of if you have your own
&quot;3rd party&quot; domains that you control, such as an images or a static
resource domain.</p>
<p>If you do, you need to make sure that two conditions apply in order for
the browser to coalesce those connections:</p>
<ul>
<li>The domain needs to be covered under the first party domain&#39;s SAN.</li>
<li>The domain needs to be DNS resolved onto the same IP as the first
party domain.</li>
</ul>
<p>Those are tricky conditions, and cannot always be satisfied (for
IT/infosec reasons), but if you can make them work, you can enjoy
connection coalescing today.</p>
<h3 id="use-credentialed-fetches-when-possible">Use credentialed fetches when possible</h3>
<p>We also mentioned that some browsers use different connection pools for
non-credentialed resources, which means even some of your first-party
resources can contend on bandwidth with your other resources.
At least when it comes to <code>XMLHttpRequest</code> and <code>fetch()</code> requests, you
can specify that these requests will be requested with credentials, and
avoid them going out on a separate connection. Unfortunately, that&#39;s not
possible for all CORS anonymous requests (e.g. there&#39;s no way to fetch
fonts as no-cors).</p>
<p>The way you would do that for XHR would be something like:</p>
<pre><code>var xhr = new XMLHttpRequest();
xhr.open(&quot;GET&quot;, &quot;https://www.example.com/data.json&quot;);
xhr.withCredentials = true;
xhr.send(null);</code></pre><p>For <code>fetch()</code>, you&#39;d need to add the credentials parameter to the init JSON:</p>
<pre><code>fetch(&quot;https://www.example.com/data.json&quot;, { credentials: &#39;include&#39; }).then((response) =&gt; {
    // Handle response
});</code></pre><h1 id="minimizing-latency">Minimizing latency</h1>
<p>One of the most important factors on resource loading performance is the
network latency. There are many aspects of the loading that are impacted
by it: connection establishment, resource discovery and the delivery itself.
While as web developers, we cannot reduce the latency of the physical network,
there are various ways in which we can get around it.</p>
<h2 id="physical-location">Physical location</h2>
<p>One way to get around the latency of the physical network is to bring
the content closer to the user. That can be done by hosting the content
on various locations around the planet and serving users from the
location closer to them. While you could do that, that will require you
to synchronize the content between those different locations.</p>
<p>Turns out, there are commercial services that will do that for you.
Content Delivery Networks (or CDNs) enable you to host your content at a
single location, while their edge servers take care of distributing it
all over the world. That enables significantly shorter end-user
latencies for both connection establishment and content delivery, 
assuming you set your content&#39;s caching headers correctly.</p>
<h2 id="caching">Caching</h2>
<p>Caching is an important part of of our fight against latency. Caching at
the origin enables you to avoid spurious server side processing,
reducing both your server-side &quot;think time&quot; as well as your CPU
requirements on the server. Caching at the edge enables you to offload
content from your origin, again serving it faster and cheaper.</p>
<p>Finally, caching at the browser enables repeat visitors to your site to
download less content, and provides them with a significantly faster and
cheaper experience.</p>
<p>There are a few essential caching policies you can employ.</p>
<h3 id="immutable">Immutable</h3>
<p>Any public content which you refer to from your pages and can change the
reference to once the content changes, should be considered immutable.
You can achieve that by have a content-addressable URL, so a URL which contains either a hash or a version of the content itself, and which changes by your build system once the content changed.
You would also need to annotate the content with something like the following headers:
<code>Cache-Control: public, immutable, max-age=315360000</code>.</p>
<p>That would tell the cache server or the browser that the content will
never change (or tell them that the content will not change in the next
10 years, if they don&#39;t support the <code>immutable</code> keyword).
It would enable them to avoid content revalidation for it, and know that
if it&#39;s in the cache and needed by the page, it can be served as is.</p>
<h3 id="always-fresh">Always fresh</h3>
<p>Any content which users navigate to directly through direct links or the
URL bar (which is usually your HTML
pages) should have a permanent URL, which does not change if the content
does. Therefore we cannot declare such content to be immutable, as it
will be impossible to modify it if we found an error in the page, a bug
or a typo.</p>
<p>You could argue that such content can be cacheable for relatively short
times (e.g. hours). Unfortunately, that would make it very hard for you
to change the content within that time window if you need to ship unpredicted changes.
As such, the safest choice, is to make sure the content gets
re-validation with the server every single time. You can do that by
using headers such as:
<code>Cache-Control: no-cache</code>.</p>
<h3 id="offload">Offload</h3>
<p>The problem with the previous approach is that it prevents origin and
edge caches from offloading the content from your origin. That means
that if the cache gets hit with 1000 requests per second, it needs to
relay those 1000 requests to the origin server, not providing much
benefit as a cache to that type of content.</p>
<p>A different approach is to provide a very short caching lifetime to your
public HTML content, giving just enough freshness to enable caches to offload
your origin. That can be achieved with something like <code>Cache-Control: public,
max-age=5</code>.</p>
<p>That will make sure your content is publicly cacheable for 5 seconds,
and gets revalidated at the origin after that. So if your cache server
gets hit with 1000 requests per second, only one request in 5000 will
get revalidated at the origin, providing significant offload and time
savings benefits.</p>
<p>Alternatively, you can use caching directives specific to shared caches,
such as <code>s-maxage</code> in order to make sure the content is cached in shared
caches for short periods of time, but will not be cached in the user&#39;s
browser cache.</p>
<h3 id="hold-till-told">Hold-till-told</h3>
<p>Another approach that is currently only feasible for origin or edge
caches is &quot;Hold till told&quot;. In short, the origin indicates the cache
server that the resource is infinitely cacheable, but reserves the right
to purge that content if a bug is found, a typo was corrected or the
wrong information was presented on that page and was then taken down.</p>
<p>The content is then present and valid in the cache for long periods of
time, does not get revalidated at the origin, but gets evicted from the
cache as soon as the origin explicitly indicates (through an API or
other proprietary means) that it should be.</p>
<p>Unfortunately, that method has no standard alternative at the moment.</p>
<h2 id="service-workers">Service Workers</h2>
<p>Another great way to reduce the impact of latency and increase the power
of caching in the browser is to use service workers.</p>
<p>A Service Worker is a JavaScript-based network proxy in the browser,
enabling the developer to inspect outgoing requests and incoming
responses and manipulate them. As such, service workers are extremely
powerful, and enable developer to go beyond the regular browser HTTP cache in caching their resources.
They enable a bunch of extremely interesting and beneficial use-cases
which weren&#39;t possible on the web before.</p>
<h3 id="offline">Offline</h3>
<p>The basic use-case which service workers cover is offline support. Since
they have access to requests and responses, as well as to the browser&#39;s
cache API, they can use that in order to cache responses as they come
in, and later use them if the user is offline. That enables sites to
create a reliable offline experience, and serve their users even in
shady or missing connectivity conditions.</p>
<p>One of the patterns that emerged if &quot;Offline first&quot;, where the
previously cached content is served to the user before going to the
network, providing them with near-instant experience, while the
resources fetched from the network are used to update the content
displayed to the user, once they arrive.</p>
<h3 id="constructing-streamed-responses">Constructing streamed responses</h3>
<p>The &quot;offline-first&quot; principle is easy to apply when your web application
is a single-page app, but is not limited to that. You can get similar
impact by caching the static parts of your HTML content in your service
worker&#39;s cache, and combine the cached content with content fetched from
your server using the Streaming API.</p>
<p>That enables the browser to start processing and displaying the static pieces of the
HTML immediately, and fill in the gaps later with the content fetched
from the server.</p>
<h3 id="compression-decoding">Compression decoding</h3>
<p>We mentioned the plans to include a compression API in the browser.
Combining such an API with Service Workers can prove to be an extremely
powerful tool. Right now, browsers have a &quot;monopoly&quot; on new compression
schemes, and previous attempts to create custom dictionary compression
on the web have failed due to deployment hurdles.
But a Service Worker based compression API can give such schemes a
second chance.</p>
<p>It can enable developers to create their own custom compression
dictionaries and use them in service workers in order to achieve
significantly better compression ratios.</p>
<h3 id="hold-till-told">Hold-till-told</h3>
<p>Another exciting aspect of Service Workers is that they enable
in-the-browser hold-till-told caching semantics. The service worker
cache can hold onto certain resources indefinitely, but purge them as
soon as instructions from the server indicate it to.</p>
<p>Such pages would need to build-in some refresh mechanism, which enables
them to update themselves if they discover they have been purged after
being served from the server. But while not being ideal, that&#39;s the
closest we can get to purge mechanisms on the web today.</p>
<h1 id="control-over-third-parties">Control over third parties</h1>
<p>One of open secrets in the web performance community is that improving
your site&#39;s performance can make little difference if you introduce third
party resources to your page that will slow it down.</p>
<p>The web&#39;s ability to mash-up content from different sources and origins
is what makes it a powerful, expressive platform. But it also comes at a
security and performance cost. Third parties on the we are often
incentivized to maximize their engagement metrics, which don&#39;t always
align with your users&#39; performance and experience.</p>
<p>The AMP project was created, at least in part, as an effort to get those third parties in
alignment with performance best-practices, by eliminating most of them
and allowing others controlled access to the page.</p>
<p>But if you&#39;re not using AMP, how can you make sure that your third party
content doesn&#39;t cause too much damage to your user&#39;s experience?</p>
<h2 id="lazy-loading">Lazy loading</h2>
<p>We talked earlier about lazy loading in the context of loading and
contention avoidance, and it is very much applicable to third party
content. Because your third party content <em>will</em> contend on bandwidth
with your own, you should delay its loading if and for as long as you possibly can,
while still allowing it to perform its function.</p>
<p>With third party content, it&#39;s particularly important not to load it in
a blocking manner. That is, not to load third party script as a blocking
<code>&lt;script&gt;</code> tag. The reason for that is that if the third party is down
for whatever reason, your site&#39;s parsing will halt until that third
party content is loaded or until the browser gives up on loading it,
which could take tens of seconds, depending on the browser and the OS.
That is usually referred to as a front-end single point of failure, or
SPOF for short.</p>
<h2 id="iframe-your-3rd-parties">Iframe your 3rd parties</h2>
<p>Another useful way to get third party content out of the way is to
compartmentalize it in its own iframe. That is good for security
(prevents it from accessing your page&#39;s DOM without your permission), as
well as for CPU performance (as cross-origin iframes have their own
main thread).</p>
<p>That&#39;s also the approach taken by AMP, where arbitrary third party
scripts can only run inside iframes.</p>
<p>Unfortunately, not all third parties are amenable to being iframed, and
some are requiring access to the main page&#39;s DOM in order to function
properly. Past initiatives such as <a href="https://www.iab.com/guidelines/safeframe/">SafeFrame</a> which would
enable controlled DOM access to iframed third parties did not take off,
and not many third parties support them.</p>
<h2 id="content-security-policy">Content-Security-Policy</h2>
<p>If you have to run your third party scripts in the main page&#39;s context,
you can restrict what content they can download and where they can
download it from using Content-Security-Policy.
Including CSP directives can help you make sure that e.g. image-based
ads don&#39;t turn into video and audio ads without your permission.
At the same time, you should note that CSP doesn’t yet allow you to
control what the iframes that load in your page are doing, so it cannot
be used to enforce restrictions on e.g. ads loaded inside iframes.</p>
<h2 id="service-worker">Service Worker</h2>
<p>We talked about Service Workers in the context of caching and offline
support, but they also enable you to enforce various rules on outgoing
requests, if they are running in the context of the main page.
For example, you can use Service Workers in order to make sure third
party content does not <a href="https://calendar.perfplanet.com/2015/reducing-single-point-of-failure-using-service-workers/">SPOF your page</a> if it doesn&#39;t load.
Similarly, you can use Service Workers in order to delay the loading of
your dynamically loaded third parties, and avoid them contending on
bandwidth and CPU with your own resources.</p>
<h2 id="feature-policy">Feature policy</h2>
<p>Finally, one of the great promises of AMP is that it only enables the
page and third parties a subset of the web&#39;s functionality. That doesn&#39;t
seem like a great promise, but in practice, it prevents both content
as well as third party developers from shooting themselves (and the
user) in the foot.</p>
<p>One effort to bring the same type of enforcement to the web is the
<a href="https://wicg.github.io/feature-policy/">Feature Policy specification</a>. It enables developers to
explicitly turn off features they know they won&#39;t need on their page,
and prevent third parties from (ab)using those features. Features like
video and audio autoplay, use of synchronous XMLHttpRequest, size of
loaded images and their compression ratios, and more. Feature policies
are inherited by iframes so the same restriction will apply to them as
well. You can also define iframe specific feature policies, so you can restrict your iframed third
parties further than you restrict the main context.</p>
<h1 id="in-summary">In summary</h1>
<p>Loading resources on the web today is hard. At the very least, it&#39;s hard
to do in an optimal way. But, it&#39;s getting better. And browsers are
heavily investing in improving it significantly.</p>
<h2 id="what-to-do-">What to do?</h2>
<p>We went through many different subjects throughout the chapter, so it&#39;s
easy to lose track of what&#39;s is theoretical and what is actionable.</p>
<p>Below is a checklist you can go over to refresh your memory, and find
things to focus on to improve your site&#39;s performance:</p>
<ul>
<li>Reduce protocol overhead<ul>
<li>Preconnect - Use preconnect to prevent connection establishment from blocking your critical path</li>
<li>TFO + TLS/1.3 or QUIC - Use cutting edge protocol stacks if you can to reduce protocol overhead</li>
</ul>
</li>
<li>Early delivery and discovery<ul>
<li>Server Push - use H2 server push to ensure early delivery of your critical resources on first views.</li>
<li>Preload - make sure your resources are early discovered by the browser.</li>
</ul>
</li>
<li>Load code <em>when</em> you need it<ul>
<li>Progressive CSS loading - Load styles for your out-of-viewport content at the point where they are needed.</li>
<li>Non-blocking JS loading - Make sure that your JS is loaded in a non-blocking manner.
your users&#39; CPU.</li>
<li>Lazy load images - Use one of the various lazy loading libraries (preferably one that uses IntersectionObservers) to lazy load your out-of-viewport images.</li>
<li>Use smart font loading strategies - Prefer FOUT and use <code>font-display: swap</code> or the Font Loading API to achieve it.</li>
</ul>
</li>
<li>Load less<ul>
<li>Avoid sending unused code - use code coverage tools to remove dead code from your JS and CSS bundles.</li>
<li>Avoid JS reliant experience - Prefer your user&#39;s experience over your own and use a small amount of non-blocking JS to avoid bogging down</li>
<li>Brotli - Use brotli compression to significantly reduce the size of your static JS/CSS files.</li>
<li>Use Responsive images and Client Hints to make sure you&#39;re not sending unnecessary image data.</li>
<li>Subset you fonts to avoid sending unused character data.</li>
</ul>
</li>
<li>Avoid bandwidth contention between your resources<ul>
<li>Rewrite static third party files to your domain</li>
<li>Dynamically delay requests for non-critical third parties.</li>
<li>Use credentialed fetches whenever possible</li>
<li>Coalesce connections using the certificate&#39;s SAN and mapping hosts
to the same IP</li>
</ul>
</li>
<li>Minimize latency<ul>
<li>Use caching headers to make sure your resources are properly
cached in the browser and on intermediary caches</li>
<li>Use a CDN to reduce latency between the user and the nearest
server</li>
<li>Service Workers can make browser caching more predictable</li>
</ul>
</li>
<li>Reduce impact of third party resources<ul>
<li>iframe and lazy load 3rd parties whenever possible</li>
</ul>
</li>
</ul>
<h2 id="what-s-in-the-future-">What&#39;s in the future?</h2>
<p>To wrap up the
chapter with an optimistic view, here&#39;s where I want loading resources
to be 5 years from now:</p>
<ul>
<li>The QUIC protocol will be standardized and universally deployed. It
will resolve many of the underlying protocol issues
that have plagued the web for a long while, and which HTTP/2 was the
first stab at fixing. It will significantly reduce the protocol overhead
and make transport significantly less sensitive to the network&#39;s
latency.</li>
<li>Third parties impact on the network will be significantly limited, as
the browser will avoid bandwidth contention between low priority
resources and high priority ones, by delaying low priority requests, and
by orchestrating the different connections and streams based on their
priority.</li>
<li>Browsers will also avoid opening multiple connections to the same
origin to prevent bandwidth contention. Secondary certs will enable
sites to easily coalesce their different domains onto the same connection for
improved prioritization.</li>
<li>The third party ecosystem will be significantly more amenable to being
iframed for increased performance and security.</li>
<li>The framework ecosystem will make heavy use of workers, delegating
more JS work off-main-thread.</li>
<li>Feature policy combined with iframes third parties will enable
developers to take back control over their user&#39;s experience.</li>
<li>Lazy loading non-critical content will become significantly easier
through new loading paradigms.</li>
<li>Early delivery and resource discovery will be fixed using Server Push
(with Cache Digests), Preload and priority hints. Build tools will
automatically create those instructions for developers at build time.</li>
<li>JavaScript resources will be bundled using WebPackages, enabling
improved compression, caching granularity, and (combined with Cache
Digests) avoiding to send down resources already in the browser&#39;s cache.</li>
<li>Common build processes will also help make sure completely unused JS
and CSS is never sent down to the browser, and rarely/late used code
is loaded later on.</li>
<li>The combination of compression APIs and Service Workers will enable
off-the-shelf delta compression solutions that will significantly
reduce the amount of framework JS and CSS users have to download.</li>
<li>Client Hints with network quality estimations will simplify server side
implementations of adaptive congestion control algorithms. QUIC wlll
make it simpler to change the congestion window after receiving the
request with those hints.</li>
<li>WebPackaging and packaged content will enable browsers to prefetch content as if it&#39;s coming
from the origin without privacy implications and side effects.</li>
<li>And finally, measurement APIs will enable us to keep track of all the
above and make sure our user&#39;s experiences are as snappy as we want
them to be.</li>
</ul>
<p>These are all things that are currently being discussed and are in
different phases of the standardization process. Some may not make it,
and be replaced with more mature incarnations. But the aim is to solve
all those problems and make the solutions for them a reality in the next
few years.</p>
<p>In aggregate, all these improvements would mean that loading resources in a performant way on the web would
become easy to do, and the default way of creating content for the web.
I believe that can make your lives as developers easier, and more
importantly improve our user&#39;s default experience of the web.</p>

